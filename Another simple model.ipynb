{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''For preprocessing images'''\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import glob\n",
    "'''For CNN'''\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, MaxPool2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 96\n",
    "train_data_size = 4000\n",
    "test_data_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(img):\n",
    "    # padding\n",
    "    longer_side = max(img.size)\n",
    "    horizontal_padding = (longer_side - img.size[0]) / 2\n",
    "    vertical_padding = (longer_side - img.size[1]) / 2\n",
    "    img = img.crop(\n",
    "        (\n",
    "            -horizontal_padding,\n",
    "            -vertical_padding,\n",
    "            img.size[0] + horizontal_padding,\n",
    "            img.size[1] + vertical_padding\n",
    "        )\n",
    "    )\n",
    "    # resizing to standardized size\n",
    "    img = img.resize([image_size,image_size],Image.ANTIALIAS) \\\n",
    "    # plt.imshow(img) # To see the image being standardized.\n",
    "    \n",
    "    # converting image to numpy array\n",
    "    img.load()\n",
    "    img = np.asarray(img, dtype=\"int32\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function():\n",
    "    for filename in glob.glob('input/subset_data/train/*.tif'):\n",
    "        img =Image.open(filename)\n",
    "        img = standardize(img)\n",
    "        print(img.shape)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading data'''\n",
    "def get_id_from_filename(filename):\n",
    "    id = filename.split(\"/\")[-1]\n",
    "    id = id.split(\".\")[0]\n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train():\n",
    "    names = []\n",
    "    # Change first number base on number of training examples\n",
    "    X_train = np.empty((train_data_size,image_size,image_size,3), dtype=\"int32\")\n",
    "    Y_train = np.empty(shape=(train_data_size,2),dtype=\"int32\")\n",
    "\n",
    "    i = 0\n",
    "    for filename in glob.glob('input/subset_data/train/*.tif'):\n",
    "        names.append(get_id_from_filename(filename))\n",
    "        img =Image.open(filename)\n",
    "        img = standardize(img)\n",
    "        X_train[i-1] = img\n",
    "        i += 1\n",
    "        \n",
    "    with open('input/subset_data/train_labels_full.csv') as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        next(readCSV, None)\n",
    "        for row in readCSV:\n",
    "            name = row[0]\n",
    "            if name in names:\n",
    "                label = int(row[1])\n",
    "                if label == 0:\n",
    "                    Y_train[names.index(name)] = np.array([1,0]) # means 0\n",
    "                elif label == 1:\n",
    "                    Y_train[names.index(name)] = np.array([0,1]) # means 1\n",
    "    return X_train,Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test():\n",
    "    names = []\n",
    "    # Change first number base on number of training examples\n",
    "    X_test = np.empty((test_data_size,image_size,image_size,3), dtype=\"int32\")\n",
    "    Y_test = np.empty(shape=(test_data_size,2),dtype=\"int32\")\n",
    "\n",
    "    i = 0\n",
    "    for filename in glob.glob('input/subset_data/test_with_outputs/*.tif'):\n",
    "        names.append(get_id_from_filename(filename))\n",
    "        img =Image.open(filename)\n",
    "        img = standardize(img)\n",
    "        X_test[i-1] = img\n",
    "        i += 1\n",
    "        \n",
    "    with open('input/subset_data/train_labels_full.csv') as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        next(readCSV, None)\n",
    "        for row in readCSV:\n",
    "            name = row[0]\n",
    "            if name in names:\n",
    "                label = int(row[1])\n",
    "                if label == 0:\n",
    "                    Y_test[names.index(name)] = np.array([1,0]) # means 0\n",
    "                elif label == 1:\n",
    "                    Y_test[names.index(name)] = np.array([0,1]) # means 1\n",
    "    return X_test,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (4000, 96, 96, 3)\n",
      "Y_train shape:  (4000, 2)\n",
      "X_test shape:  (1000, 96, 96, 3)\n",
      "Y_test shape:  (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train_orig,Y_train_orig = load_train()\n",
    "X_test_orig,Y_test_orig = load_test()\n",
    "\n",
    "# Normalizing for faster convergence\n",
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "Y_train = Y_train_orig\n",
    "Y_test = Y_test_orig\n",
    "\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"Y_train shape: \", Y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"Y_test shape: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.89019608, 0.89411765, 0.90196078],\n",
       "         [0.89019608, 0.89411765, 0.90196078],\n",
       "         [0.89019608, 0.89411765, 0.90196078],\n",
       "         ...,\n",
       "         [0.9254902 , 0.8627451 , 0.90588235],\n",
       "         [0.88627451, 0.80392157, 0.8627451 ],\n",
       "         [0.85098039, 0.76078431, 0.83137255]],\n",
       "\n",
       "        [[0.89019608, 0.89411765, 0.90196078],\n",
       "         [0.89019608, 0.89411765, 0.90196078],\n",
       "         [0.89019608, 0.89411765, 0.90196078],\n",
       "         ...,\n",
       "         [0.83921569, 0.76862745, 0.83137255],\n",
       "         [0.84313725, 0.76078431, 0.82745098],\n",
       "         [0.83529412, 0.75294118, 0.82745098]],\n",
       "\n",
       "        [[0.89019608, 0.89411765, 0.90196078],\n",
       "         [0.89019608, 0.89411765, 0.90196078],\n",
       "         [0.89019608, 0.89411765, 0.90196078],\n",
       "         ...,\n",
       "         [0.82745098, 0.75294118, 0.82745098],\n",
       "         [0.82745098, 0.75294118, 0.83137255],\n",
       "         [0.83529412, 0.76078431, 0.83921569]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1.        , 0.96470588, 0.97254902],\n",
       "         [1.        , 0.96470588, 1.        ],\n",
       "         [1.        , 0.91372549, 1.        ],\n",
       "         ...,\n",
       "         [0.84705882, 0.72941176, 0.83921569],\n",
       "         [0.85490196, 0.7372549 , 0.83921569],\n",
       "         [0.87058824, 0.74509804, 0.83921569]],\n",
       "\n",
       "        [[1.        , 0.95294118, 0.96078431],\n",
       "         [0.94901961, 0.84313725, 0.90196078],\n",
       "         [0.62352941, 0.48627451, 0.61176471],\n",
       "         ...,\n",
       "         [0.91372549, 0.79215686, 0.91372549],\n",
       "         [0.8745098 , 0.74117647, 0.84705882],\n",
       "         [0.89019608, 0.74901961, 0.85882353]],\n",
       "\n",
       "        [[0.9254902 , 0.82745098, 0.84313725],\n",
       "         [0.47058824, 0.34901961, 0.41176471],\n",
       "         [0.49411765, 0.32941176, 0.47843137],\n",
       "         ...,\n",
       "         [0.87843137, 0.75686275, 0.87843137],\n",
       "         [0.87058824, 0.73333333, 0.85098039],\n",
       "         [0.90980392, 0.75686275, 0.87058824]]],\n",
       "\n",
       "\n",
       "       [[[0.57254902, 0.4627451 , 0.50980392],\n",
       "         [0.85098039, 0.74117647, 0.78823529],\n",
       "         [0.95294118, 0.84313725, 0.89019608],\n",
       "         ...,\n",
       "         [0.84705882, 0.76470588, 0.79215686],\n",
       "         [1.        , 0.97254902, 0.99607843],\n",
       "         [0.90196078, 0.83529412, 0.85882353]],\n",
       "\n",
       "        [[0.98431373, 0.88235294, 0.9254902 ],\n",
       "         [1.        , 0.9372549 , 0.98039216],\n",
       "         [0.73333333, 0.63137255, 0.6745098 ],\n",
       "         ...,\n",
       "         [0.94509804, 0.8627451 , 0.89019608],\n",
       "         [0.88627451, 0.81176471, 0.83529412],\n",
       "         [0.89803922, 0.83137255, 0.85490196]],\n",
       "\n",
       "        [[1.        , 0.95294118, 0.99607843],\n",
       "         [0.8745098 , 0.77254902, 0.81568627],\n",
       "         [1.        , 0.89803922, 0.94117647],\n",
       "         ...,\n",
       "         [0.96862745, 0.87843137, 0.91764706],\n",
       "         [1.        , 0.94509804, 0.96862745],\n",
       "         [0.85490196, 0.78039216, 0.80392157]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.97254902, 0.96470588, 0.96862745],\n",
       "         [0.97647059, 0.96862745, 0.97254902],\n",
       "         [0.98039216, 0.97254902, 0.97647059],\n",
       "         ...,\n",
       "         [1.        , 0.90588235, 0.96078431],\n",
       "         [0.82745098, 0.65882353, 0.7372549 ],\n",
       "         [0.96862745, 0.75686275, 0.85098039]],\n",
       "\n",
       "        [[0.97254902, 0.96470588, 0.96862745],\n",
       "         [0.97647059, 0.96862745, 0.97254902],\n",
       "         [0.98039216, 0.97254902, 0.97647059],\n",
       "         ...,\n",
       "         [0.94509804, 0.82352941, 0.88235294],\n",
       "         [0.74117647, 0.55294118, 0.63921569],\n",
       "         [0.81176471, 0.58039216, 0.69019608]],\n",
       "\n",
       "        [[0.97254902, 0.96470588, 0.96862745],\n",
       "         [0.97647059, 0.96862745, 0.97254902],\n",
       "         [0.98039216, 0.97254902, 0.97647059],\n",
       "         ...,\n",
       "         [0.98431373, 0.85882353, 0.91764706],\n",
       "         [0.74509804, 0.55294118, 0.63921569],\n",
       "         [0.72941176, 0.49411765, 0.60392157]]],\n",
       "\n",
       "\n",
       "       [[[0.79215686, 0.65490196, 0.73333333],\n",
       "         [0.98039216, 0.83137255, 0.91372549],\n",
       "         [0.90196078, 0.7372549 , 0.82352941],\n",
       "         ...,\n",
       "         [0.99215686, 0.98431373, 1.        ],\n",
       "         [0.84705882, 0.80392157, 0.83529412],\n",
       "         [1.        , 0.97254902, 1.        ]],\n",
       "\n",
       "        [[0.8745098 , 0.72156863, 0.79215686],\n",
       "         [0.9254902 , 0.76470588, 0.83529412],\n",
       "         [0.86666667, 0.69803922, 0.77647059],\n",
       "         ...,\n",
       "         [0.98039216, 0.96078431, 0.97647059],\n",
       "         [0.6745098 , 0.63137255, 0.6627451 ],\n",
       "         [1.        , 0.96862745, 1.        ]],\n",
       "\n",
       "        [[0.81568627, 0.64705882, 0.71372549],\n",
       "         [0.85882353, 0.68627451, 0.75294118],\n",
       "         [0.94901961, 0.76470588, 0.83529412],\n",
       "         ...,\n",
       "         [0.89803922, 0.8745098 , 0.88235294],\n",
       "         [0.92156863, 0.87843137, 0.89411765],\n",
       "         [0.96078431, 0.90196078, 0.92941176]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.87058824, 0.75294118, 0.79215686],\n",
       "         [0.94117647, 0.82352941, 0.8627451 ],\n",
       "         [0.9372549 , 0.81960784, 0.85882353],\n",
       "         ...,\n",
       "         [0.76470588, 0.67843137, 0.7254902 ],\n",
       "         [0.87058824, 0.76470588, 0.82352941],\n",
       "         [0.83921569, 0.71764706, 0.78823529]],\n",
       "\n",
       "        [[0.85882353, 0.74117647, 0.78039216],\n",
       "         [0.90588235, 0.78823529, 0.82745098],\n",
       "         [0.95294118, 0.83529412, 0.8745098 ],\n",
       "         ...,\n",
       "         [0.78823529, 0.68627451, 0.72941176],\n",
       "         [0.86666667, 0.74509804, 0.80392157],\n",
       "         [0.93333333, 0.8       , 0.8627451 ]],\n",
       "\n",
       "        [[0.78431373, 0.66666667, 0.70588235],\n",
       "         [0.83137255, 0.71372549, 0.75294118],\n",
       "         [0.8745098 , 0.76862745, 0.80392157],\n",
       "         ...,\n",
       "         [0.85098039, 0.74509804, 0.78039216],\n",
       "         [0.94117647, 0.81176471, 0.85490196],\n",
       "         [0.83529412, 0.68627451, 0.74509804]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.92941176, 0.8745098 , 0.94117647],\n",
       "         [0.83137255, 0.76470588, 0.83529412],\n",
       "         [0.76862745, 0.69411765, 0.76862745],\n",
       "         ...,\n",
       "         [0.69019608, 0.37254902, 0.63529412],\n",
       "         [0.89803922, 0.58823529, 0.84313725],\n",
       "         [0.95686275, 0.68235294, 0.9254902 ]],\n",
       "\n",
       "        [[0.98431373, 0.91764706, 0.99215686],\n",
       "         [0.71372549, 0.63921569, 0.71764706],\n",
       "         [0.91764706, 0.84313725, 0.92156863],\n",
       "         ...,\n",
       "         [0.98039216, 0.65882353, 0.91764706],\n",
       "         [0.93333333, 0.61568627, 0.8745098 ],\n",
       "         [0.94509804, 0.6627451 , 0.90980392]],\n",
       "\n",
       "        [[0.98823529, 0.90196078, 0.99215686],\n",
       "         [0.76470588, 0.67843137, 0.76862745],\n",
       "         [0.91764706, 0.82352941, 0.91764706],\n",
       "         ...,\n",
       "         [0.74901961, 0.41568627, 0.67058824],\n",
       "         [0.90980392, 0.58823529, 0.83921569],\n",
       "         [0.65490196, 0.35686275, 0.6       ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.89411765, 0.6627451 , 0.85098039],\n",
       "         [0.84313725, 0.58823529, 0.8       ],\n",
       "         [0.87058824, 0.59607843, 0.83137255],\n",
       "         ...,\n",
       "         [0.89411765, 0.76078431, 0.86666667],\n",
       "         [0.90588235, 0.77647059, 0.88235294],\n",
       "         [0.95294118, 0.83529412, 0.94509804]],\n",
       "\n",
       "        [[0.85490196, 0.62352941, 0.80392157],\n",
       "         [0.78823529, 0.5372549 , 0.74117647],\n",
       "         [0.8627451 , 0.59607843, 0.82745098],\n",
       "         ...,\n",
       "         [0.81176471, 0.70196078, 0.78823529],\n",
       "         [0.88627451, 0.79607843, 0.86666667],\n",
       "         [0.87843137, 0.79607843, 0.8627451 ]],\n",
       "\n",
       "        [[0.87843137, 0.64705882, 0.82745098],\n",
       "         [0.91372549, 0.6627451 , 0.86666667],\n",
       "         [0.74117647, 0.4745098 , 0.70588235],\n",
       "         ...,\n",
       "         [0.69411765, 0.60392157, 0.6745098 ],\n",
       "         [0.92156863, 0.85098039, 0.89803922],\n",
       "         [0.92941176, 0.86666667, 0.90588235]]],\n",
       "\n",
       "\n",
       "       [[[0.70196078, 0.44705882, 0.58039216],\n",
       "         [0.84705882, 0.59215686, 0.71764706],\n",
       "         [0.59607843, 0.34117647, 0.46666667],\n",
       "         ...,\n",
       "         [0.67058824, 0.45098039, 0.62745098],\n",
       "         [0.44705882, 0.23137255, 0.39607843],\n",
       "         [0.51764706, 0.30980392, 0.4745098 ]],\n",
       "\n",
       "        [[0.71372549, 0.45882353, 0.61568627],\n",
       "         [0.74117647, 0.49803922, 0.64313725],\n",
       "         [0.7254902 , 0.48235294, 0.62745098],\n",
       "         ...,\n",
       "         [0.50980392, 0.29019608, 0.46666667],\n",
       "         [0.48627451, 0.27058824, 0.43529412],\n",
       "         [0.40392157, 0.18823529, 0.35294118]],\n",
       "\n",
       "        [[0.61568627, 0.38431373, 0.56470588],\n",
       "         [0.59215686, 0.36470588, 0.5254902 ],\n",
       "         [0.50980392, 0.29019608, 0.43529412],\n",
       "         ...,\n",
       "         [0.48235294, 0.2745098 , 0.44705882],\n",
       "         [0.64705882, 0.43137255, 0.59607843],\n",
       "         [0.65490196, 0.43921569, 0.60392157]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.92941176, 0.74509804, 0.72156863],\n",
       "         [0.94117647, 0.76862745, 0.76470588],\n",
       "         [0.7372549 , 0.58431373, 0.60392157],\n",
       "         ...,\n",
       "         [0.79215686, 0.58431373, 0.69411765],\n",
       "         [0.80784314, 0.61176471, 0.71764706],\n",
       "         [0.74901961, 0.56078431, 0.6627451 ]],\n",
       "\n",
       "        [[0.69803922, 0.48235294, 0.50196078],\n",
       "         [0.89803922, 0.69019608, 0.72941176],\n",
       "         [0.82352941, 0.63529412, 0.69803922],\n",
       "         ...,\n",
       "         [0.90196078, 0.70588235, 0.81960784],\n",
       "         [0.75294118, 0.56078431, 0.6745098 ],\n",
       "         [0.71764706, 0.5372549 , 0.64705882]],\n",
       "\n",
       "        [[0.78823529, 0.5372549 , 0.6       ],\n",
       "         [0.8627451 , 0.62352941, 0.69411765],\n",
       "         [0.95294118, 0.73333333, 0.83921569],\n",
       "         ...,\n",
       "         [0.95294118, 0.75686275, 0.87058824],\n",
       "         [0.77647059, 0.58431373, 0.69803922],\n",
       "         [0.66666667, 0.48627451, 0.59607843]]],\n",
       "\n",
       "\n",
       "       [[[0.29803922, 0.14509804, 0.50196078],\n",
       "         [0.60784314, 0.42745098, 0.77254902],\n",
       "         [0.47058824, 0.23921569, 0.56078431],\n",
       "         ...,\n",
       "         [0.65098039, 0.43137255, 0.70980392],\n",
       "         [0.50196078, 0.33333333, 0.60392157],\n",
       "         [0.46666667, 0.32941176, 0.59607843]],\n",
       "\n",
       "        [[0.31764706, 0.18823529, 0.52941176],\n",
       "         [0.38431373, 0.21960784, 0.55686275],\n",
       "         [0.64313725, 0.42352941, 0.76470588],\n",
       "         ...,\n",
       "         [0.65098039, 0.40784314, 0.70980392],\n",
       "         [0.40392157, 0.20392157, 0.50588235],\n",
       "         [0.53333333, 0.36078431, 0.6627451 ]],\n",
       "\n",
       "        [[0.41960784, 0.32941176, 0.65098039],\n",
       "         [0.27058824, 0.1372549 , 0.4745098 ],\n",
       "         [0.49411765, 0.30196078, 0.6627451 ],\n",
       "         ...,\n",
       "         [0.84313725, 0.55686275, 0.87843137],\n",
       "         [0.46666667, 0.20784314, 0.54901961],\n",
       "         [0.5372549 , 0.30196078, 0.64705882]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.49019608, 0.37254902, 0.70980392],\n",
       "         [0.51372549, 0.36862745, 0.6745098 ],\n",
       "         [0.59215686, 0.42745098, 0.67843137],\n",
       "         ...,\n",
       "         [0.36862745, 0.2       , 0.54901961],\n",
       "         [0.41176471, 0.25098039, 0.61176471],\n",
       "         [0.54117647, 0.38039216, 0.74901961]],\n",
       "\n",
       "        [[0.52941176, 0.36078431, 0.6745098 ],\n",
       "         [0.61568627, 0.43529412, 0.71764706],\n",
       "         [0.7372549 , 0.54901961, 0.77647059],\n",
       "         ...,\n",
       "         [0.67058824, 0.46666667, 0.78823529],\n",
       "         [0.60392157, 0.40784314, 0.74901961],\n",
       "         [0.44705882, 0.24705882, 0.60784314]],\n",
       "\n",
       "        [[0.57647059, 0.37647059, 0.67058824],\n",
       "         [0.5372549 , 0.32941176, 0.59607843],\n",
       "         [0.84705882, 0.65490196, 0.87058824],\n",
       "         ...,\n",
       "         [0.59215686, 0.36862745, 0.67058824],\n",
       "         [0.43529412, 0.20784314, 0.5372549 ],\n",
       "         [0.55294118, 0.3254902 , 0.66666667]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[227. 228. 230.]\n",
      "   [227. 228. 230.]\n",
      "   [227. 228. 230.]\n",
      "   ...\n",
      "   [236. 220. 231.]\n",
      "   [226. 205. 220.]\n",
      "   [217. 194. 212.]]\n",
      "\n",
      "  [[227. 228. 230.]\n",
      "   [227. 228. 230.]\n",
      "   [227. 228. 230.]\n",
      "   ...\n",
      "   [214. 196. 212.]\n",
      "   [215. 194. 211.]\n",
      "   [213. 192. 211.]]\n",
      "\n",
      "  [[227. 228. 230.]\n",
      "   [227. 228. 230.]\n",
      "   [227. 228. 230.]\n",
      "   ...\n",
      "   [211. 192. 211.]\n",
      "   [211. 192. 212.]\n",
      "   [213. 194. 214.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[255. 246. 248.]\n",
      "   [255. 246. 255.]\n",
      "   [255. 233. 255.]\n",
      "   ...\n",
      "   [216. 186. 214.]\n",
      "   [218. 188. 214.]\n",
      "   [222. 190. 214.]]\n",
      "\n",
      "  [[255. 243. 245.]\n",
      "   [242. 215. 230.]\n",
      "   [159. 124. 156.]\n",
      "   ...\n",
      "   [233. 202. 233.]\n",
      "   [223. 189. 216.]\n",
      "   [227. 191. 219.]]\n",
      "\n",
      "  [[236. 211. 215.]\n",
      "   [120.  89. 105.]\n",
      "   [126.  84. 122.]\n",
      "   ...\n",
      "   [224. 193. 224.]\n",
      "   [222. 187. 217.]\n",
      "   [232. 193. 222.]]]\n",
      "\n",
      "\n",
      " [[[146. 118. 130.]\n",
      "   [217. 189. 201.]\n",
      "   [243. 215. 227.]\n",
      "   ...\n",
      "   [216. 195. 202.]\n",
      "   [255. 248. 254.]\n",
      "   [230. 213. 219.]]\n",
      "\n",
      "  [[251. 225. 236.]\n",
      "   [255. 239. 250.]\n",
      "   [187. 161. 172.]\n",
      "   ...\n",
      "   [241. 220. 227.]\n",
      "   [226. 207. 213.]\n",
      "   [229. 212. 218.]]\n",
      "\n",
      "  [[255. 243. 254.]\n",
      "   [223. 197. 208.]\n",
      "   [255. 229. 240.]\n",
      "   ...\n",
      "   [247. 224. 234.]\n",
      "   [255. 241. 247.]\n",
      "   [218. 199. 205.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[248. 246. 247.]\n",
      "   [249. 247. 248.]\n",
      "   [250. 248. 249.]\n",
      "   ...\n",
      "   [255. 231. 245.]\n",
      "   [211. 168. 188.]\n",
      "   [247. 193. 217.]]\n",
      "\n",
      "  [[248. 246. 247.]\n",
      "   [249. 247. 248.]\n",
      "   [250. 248. 249.]\n",
      "   ...\n",
      "   [241. 210. 225.]\n",
      "   [189. 141. 163.]\n",
      "   [207. 148. 176.]]\n",
      "\n",
      "  [[248. 246. 247.]\n",
      "   [249. 247. 248.]\n",
      "   [250. 248. 249.]\n",
      "   ...\n",
      "   [251. 219. 234.]\n",
      "   [190. 141. 163.]\n",
      "   [186. 126. 154.]]]\n",
      "\n",
      "\n",
      " [[[202. 167. 187.]\n",
      "   [250. 212. 233.]\n",
      "   [230. 188. 210.]\n",
      "   ...\n",
      "   [253. 251. 255.]\n",
      "   [216. 205. 213.]\n",
      "   [255. 248. 255.]]\n",
      "\n",
      "  [[223. 184. 202.]\n",
      "   [236. 195. 213.]\n",
      "   [221. 178. 198.]\n",
      "   ...\n",
      "   [250. 245. 249.]\n",
      "   [172. 161. 169.]\n",
      "   [255. 247. 255.]]\n",
      "\n",
      "  [[208. 165. 182.]\n",
      "   [219. 175. 192.]\n",
      "   [242. 195. 213.]\n",
      "   ...\n",
      "   [229. 223. 225.]\n",
      "   [235. 224. 228.]\n",
      "   [245. 230. 237.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[222. 192. 202.]\n",
      "   [240. 210. 220.]\n",
      "   [239. 209. 219.]\n",
      "   ...\n",
      "   [195. 173. 185.]\n",
      "   [222. 195. 210.]\n",
      "   [214. 183. 201.]]\n",
      "\n",
      "  [[219. 189. 199.]\n",
      "   [231. 201. 211.]\n",
      "   [243. 213. 223.]\n",
      "   ...\n",
      "   [201. 175. 186.]\n",
      "   [221. 190. 205.]\n",
      "   [238. 204. 220.]]\n",
      "\n",
      "  [[200. 170. 180.]\n",
      "   [212. 182. 192.]\n",
      "   [223. 196. 205.]\n",
      "   ...\n",
      "   [217. 190. 199.]\n",
      "   [240. 207. 218.]\n",
      "   [213. 175. 190.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[237. 223. 240.]\n",
      "   [212. 195. 213.]\n",
      "   [196. 177. 196.]\n",
      "   ...\n",
      "   [176.  95. 162.]\n",
      "   [229. 150. 215.]\n",
      "   [244. 174. 236.]]\n",
      "\n",
      "  [[251. 234. 253.]\n",
      "   [182. 163. 183.]\n",
      "   [234. 215. 235.]\n",
      "   ...\n",
      "   [250. 168. 234.]\n",
      "   [238. 157. 223.]\n",
      "   [241. 169. 232.]]\n",
      "\n",
      "  [[252. 230. 253.]\n",
      "   [195. 173. 196.]\n",
      "   [234. 210. 234.]\n",
      "   ...\n",
      "   [191. 106. 171.]\n",
      "   [232. 150. 214.]\n",
      "   [167.  91. 153.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[228. 169. 217.]\n",
      "   [215. 150. 204.]\n",
      "   [222. 152. 212.]\n",
      "   ...\n",
      "   [228. 194. 221.]\n",
      "   [231. 198. 225.]\n",
      "   [243. 213. 241.]]\n",
      "\n",
      "  [[218. 159. 205.]\n",
      "   [201. 137. 189.]\n",
      "   [220. 152. 211.]\n",
      "   ...\n",
      "   [207. 179. 201.]\n",
      "   [226. 203. 221.]\n",
      "   [224. 203. 220.]]\n",
      "\n",
      "  [[224. 165. 211.]\n",
      "   [233. 169. 221.]\n",
      "   [189. 121. 180.]\n",
      "   ...\n",
      "   [177. 154. 172.]\n",
      "   [235. 217. 229.]\n",
      "   [237. 221. 231.]]]\n",
      "\n",
      "\n",
      " [[[179. 114. 148.]\n",
      "   [216. 151. 183.]\n",
      "   [152.  87. 119.]\n",
      "   ...\n",
      "   [171. 115. 160.]\n",
      "   [114.  59. 101.]\n",
      "   [132.  79. 121.]]\n",
      "\n",
      "  [[182. 117. 157.]\n",
      "   [189. 127. 164.]\n",
      "   [185. 123. 160.]\n",
      "   ...\n",
      "   [130.  74. 119.]\n",
      "   [124.  69. 111.]\n",
      "   [103.  48.  90.]]\n",
      "\n",
      "  [[157.  98. 144.]\n",
      "   [151.  93. 134.]\n",
      "   [130.  74. 111.]\n",
      "   ...\n",
      "   [123.  70. 114.]\n",
      "   [165. 110. 152.]\n",
      "   [167. 112. 154.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[237. 190. 184.]\n",
      "   [240. 196. 195.]\n",
      "   [188. 149. 154.]\n",
      "   ...\n",
      "   [202. 149. 177.]\n",
      "   [206. 156. 183.]\n",
      "   [191. 143. 169.]]\n",
      "\n",
      "  [[178. 123. 128.]\n",
      "   [229. 176. 186.]\n",
      "   [210. 162. 178.]\n",
      "   ...\n",
      "   [230. 180. 209.]\n",
      "   [192. 143. 172.]\n",
      "   [183. 137. 165.]]\n",
      "\n",
      "  [[201. 137. 153.]\n",
      "   [220. 159. 177.]\n",
      "   [243. 187. 214.]\n",
      "   ...\n",
      "   [243. 193. 222.]\n",
      "   [198. 149. 178.]\n",
      "   [170. 124. 152.]]]\n",
      "\n",
      "\n",
      " [[[ 76.  37. 128.]\n",
      "   [155. 109. 197.]\n",
      "   [120.  61. 143.]\n",
      "   ...\n",
      "   [166. 110. 181.]\n",
      "   [128.  85. 154.]\n",
      "   [119.  84. 152.]]\n",
      "\n",
      "  [[ 81.  48. 135.]\n",
      "   [ 98.  56. 142.]\n",
      "   [164. 108. 195.]\n",
      "   ...\n",
      "   [166. 104. 181.]\n",
      "   [103.  52. 129.]\n",
      "   [136.  92. 169.]]\n",
      "\n",
      "  [[107.  84. 166.]\n",
      "   [ 69.  35. 121.]\n",
      "   [126.  77. 169.]\n",
      "   ...\n",
      "   [215. 142. 224.]\n",
      "   [119.  53. 140.]\n",
      "   [137.  77. 165.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[125.  95. 181.]\n",
      "   [131.  94. 172.]\n",
      "   [151. 109. 173.]\n",
      "   ...\n",
      "   [ 94.  51. 140.]\n",
      "   [105.  64. 156.]\n",
      "   [138.  97. 191.]]\n",
      "\n",
      "  [[135.  92. 172.]\n",
      "   [157. 111. 183.]\n",
      "   [188. 140. 198.]\n",
      "   ...\n",
      "   [171. 119. 201.]\n",
      "   [154. 104. 191.]\n",
      "   [114.  63. 155.]]\n",
      "\n",
      "  [[147.  96. 171.]\n",
      "   [137.  84. 152.]\n",
      "   [216. 167. 222.]\n",
      "   ...\n",
      "   [151.  94. 171.]\n",
      "   [111.  53. 137.]\n",
      "   [141.  83. 170.]]]]\n"
     ]
    }
   ],
   "source": [
    "# To check values inside.\n",
    "# print(X_train)\n",
    "# print(Y_train)\n",
    "# print(X_test)\n",
    "# print(Y_test)\n",
    "X_test_orig = X_test*255.0\n",
    "print(X_train_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tried and tested before:\n",
    "https://towardsdatascience.com/image-classification-python-keras-tutorial-kaggle-challenge-45a6332a58b8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size = (3, 3), activation='relu', input_shape=(image_size, image_size, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(96, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 94, 94, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 47, 47, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 47, 47, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 45, 45, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 22, 22, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 20, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 96)          55392     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 96)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 96)          384       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 2, 2, 32)          27680     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1, 1, 32)          128       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               4224      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 145,026\n",
      "Trainable params: 144,450\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "4000/4000 [==============================] - 6s 2ms/step - loss: 0.7021 - acc: 0.5667\n",
      "Epoch 2/20\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6828 - acc: 0.5787\n",
      "Epoch 3/20\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6861 - acc: 0.5740\n",
      "Epoch 4/20\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6843 - acc: 0.5757\n",
      "Epoch 5/20\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6840 - acc: 0.5795\n",
      "Epoch 6/20\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6777 - acc: 0.5867\n",
      "Epoch 7/20\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6834 - acc: 0.5755\n",
      "Epoch 8/20\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6829 - acc: 0.5863\n",
      "Epoch 9/20\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6805 - acc: 0.5787\n",
      "Epoch 10/20\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6794 - acc: 0.5897\n",
      "Epoch 11/20\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6789 - acc: 0.5897\n",
      "Epoch 12/20\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6789 - acc: 0.5893\n",
      "Epoch 13/20\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6767 - acc: 0.5853\n",
      "Epoch 14/20\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6751 - acc: 0.5907\n",
      "Epoch 15/20\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6750 - acc: 0.5840\n",
      "Epoch 16/20\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6693 - acc: 0.6105\n",
      "Epoch 17/20\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6608 - acc: 0.6108\n",
      "Epoch 18/20\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6618 - acc: 0.6090\n",
      "Epoch 19/20\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6393 - acc: 0.6407\n",
      "Epoch 20/20\n",
      "1824/4000 [============>.................] - ETA: 2s - loss: 0.6025 - acc: 0.6826"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size = 32, epochs = 20, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.evaluate(x = X_test, y = Y_test)\n",
    "print()\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
