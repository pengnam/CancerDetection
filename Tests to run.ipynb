{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''For preprocessing images'''\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import glob\n",
    "'''For CNN'''\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune some paramters here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 96\n",
    "train_data_size = 4000\n",
    "test_data_size = 1000\n",
    "process = lambda x:(x - x.mean()) / x.std() if x.std() > 0 else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(img):\n",
    "    # padding\n",
    "    longer_side = max(img.size)\n",
    "    horizontal_padding = (longer_side - img.size[0]) / 2\n",
    "    vertical_padding = (longer_side - img.size[1]) / 2\n",
    "    img = img.crop(\n",
    "        (\n",
    "            -horizontal_padding,\n",
    "            -vertical_padding,\n",
    "            img.size[0] + horizontal_padding,\n",
    "            img.size[1] + vertical_padding\n",
    "        )\n",
    "    )\n",
    "    # resizing to standardized size\n",
    "    img = img.resize([image_size,image_size],Image.ANTIALIAS) \\\n",
    "    # plt.imshow(img) # To see the image being standardized.\n",
    "    \n",
    "    # converting image to numpy array\n",
    "    img.load()\n",
    "    img = np.asarray(img, dtype=\"int32\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function():\n",
    "    for filename in glob.glob('input/subset_data/train/*.tif'):\n",
    "        img =Image.open(filename)\n",
    "        img = standardize(img)\n",
    "        print(img.shape)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading data'''\n",
    "def get_id_from_filename(filename):\n",
    "    id = filename.split(\"/\")[-1]\n",
    "    id = id.split(\".\")[0]\n",
    "    return id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit Code:\n",
    "- If number of training example changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train():\n",
    "    names = []\n",
    "    # Change first number base on number of training examples\n",
    "    X_train = np.empty((train_data_size,image_size,image_size,3))\n",
    "    Y_train = np.empty(shape=(train_data_size,1))\n",
    "\n",
    "    i = 0\n",
    "    for filename in glob.glob('input/subset_data/train/*.tif'):\n",
    "        names.append(get_id_from_filename(filename))\n",
    "        img = Image.open(filename)\n",
    "        img.load()\n",
    "        img =np.asarray(img)\n",
    "        img = process(img)\n",
    "        X_train[i-1] = img\n",
    "        i += 1\n",
    "        \n",
    "    with open('input/subset_data/train_labels_full.csv') as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        next(readCSV, None)\n",
    "        for row in readCSV:\n",
    "            name = row[0]\n",
    "            if name in names:\n",
    "                label = int(row[1])\n",
    "                if label == 0:\n",
    "                    Y_train[names.index(name)] = np.array([0]) # means 0\n",
    "                elif label == 1:\n",
    "                    Y_train[names.index(name)] = np.array([1]) # means 1\n",
    "    return X_train,Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit Code:\n",
    "- If number of training example changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test():\n",
    "    names = []\n",
    "    # Change first number base on number of training examples\n",
    "    X_test = np.empty((test_data_size,image_size,image_size,3))\n",
    "    Y_test = np.empty(shape=(test_data_size,1))\n",
    "\n",
    "    i = 0\n",
    "    for filename in glob.glob('input/subset_data/test_with_outputs/*.tif'):\n",
    "        names.append(get_id_from_filename(filename))\n",
    "        img = Image.open(filename)\n",
    "        img.load()\n",
    "        img =np.asarray(img)\n",
    "        img = process(img)\n",
    "        X_test[i-1] = img\n",
    "        i += 1\n",
    "        \n",
    "    with open('input/subset_data/train_labels_full.csv') as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        next(readCSV, None)\n",
    "        for row in readCSV:\n",
    "            name = row[0]\n",
    "            if name in names:\n",
    "                label = int(row[1])\n",
    "                if label == 0:\n",
    "                    Y_test[names.index(name)] = np.array([0]) # means 0\n",
    "                elif label == 1:\n",
    "                    Y_test[names.index(name)] = np.array([1]) # means 1\n",
    "    return X_test,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (4000, 96, 96, 3)\n",
      "Y_train shape:  (4000, 1)\n",
      "X_test shape:  (1000, 96, 96, 3)\n",
      "Y_test shape:  (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train,Y_train = load_train()\n",
    "X_test,Y_test = load_test()\n",
    "\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"Y_train shape: \", Y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"Y_test shape: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check values inside.\n",
    "# print(X_train)\n",
    "# print(Y_train)\n",
    "# print(X_test)\n",
    "# print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainingModel(input_shape):\n",
    "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Zero-Padding: pads the border of X_input with zeroes\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    X = Conv2D(8, (4, 4), strides = (1, 1), name = 'conv0')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D((8, 8), name='max_pool0')(X)\n",
    "    \n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    X = Conv2D(16, (2, 2), strides = (1, 1), name = 'conv1')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D((4, 4), name='max_pool1')(X)\n",
    "\n",
    "    # FLATTEN X (means convert it to a vector) + FULLYCONNECTED\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(1, activation='sigmoid', name='fc')(X)\n",
    "\n",
    "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    model = Model(inputs = X_input, outputs = X, name='trainingModel')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "trainingModel = TrainingModel((X_train.shape[1],X_train.shape[2],X_train.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 96, 96, 3)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 102, 102, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv2D)               (None, 99, 99, 8)         392       \n",
      "_________________________________________________________________\n",
      "bn0 (BatchNormalization)     (None, 99, 99, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 99, 99, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pool0 (MaxPooling2D)     (None, 12, 12, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 11, 11, 16)        528       \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (None, 11, 11, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 11, 11, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pool1 (MaxPooling2D)     (None, 2, 2, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,081\n",
      "Trainable params: 1,033\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trainingModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingModel.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "# trainingModel.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "4000/4000 [==============================] - 3s 832us/step - loss: 0.7338 - acc: 0.5465 - val_loss: 0.7068 - val_acc: 0.5290\n",
      "Epoch 2/50\n",
      "4000/4000 [==============================] - 2s 456us/step - loss: 0.6923 - acc: 0.5697 - val_loss: 0.6849 - val_acc: 0.5840\n",
      "Epoch 3/50\n",
      "4000/4000 [==============================] - 2s 463us/step - loss: 0.6813 - acc: 0.5827 - val_loss: 0.6805 - val_acc: 0.5960\n",
      "Epoch 4/50\n",
      "4000/4000 [==============================] - 2s 448us/step - loss: 0.6722 - acc: 0.5855 - val_loss: 0.6947 - val_acc: 0.5510\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 5/50\n",
      "4000/4000 [==============================] - 2s 452us/step - loss: 0.6578 - acc: 0.6130 - val_loss: 0.6863 - val_acc: 0.5780\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f19bd5d2278>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1, restore_best_weights=True)\n",
    "reducel = ReduceLROnPlateau(monitor='val_loss', patience=1, verbose=1, factor=0.1)\n",
    "\n",
    "trainingModel.fit(x = X_train, y = Y_train, epochs = 50, batch_size = 32, validation_data=(X_test,Y_test),callbacks=[reducel, earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 210us/step\n",
      "\n",
      "Loss = 0.6804630780220031\n",
      "Test Accuracy = 0.596\n"
     ]
    }
   ],
   "source": [
    "preds = trainingModel.evaluate(x = X_test, y = Y_test)\n",
    "print()\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A deeper CNN\n",
    "https://towardsdatascience.com/image-classification-python-keras-tutorial-kaggle-challenge-45a6332a58b8\n",
    "## 5 ways to improve model:\n",
    "- adding more dropout layers (for overfitting)\n",
    "- experiment with removing or adding convolutional layers, changing the filter size, or even changing the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(ZeroPadding2D((3, 3)))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size = (3, 3), activation='relu', input_shape=(image_size, image_size, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2)) # newly added\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2)) # newly added\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2)) # newly added\n",
    "\n",
    "model.add(Conv2D(96, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2)) # newly added\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))  # newly added\n",
    "model.add(Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "4000/4000 [==============================] - 8s 2ms/step - loss: 0.7421 - acc: 0.5410 - val_loss: 0.7071 - val_acc: 0.5830\n",
      "Epoch 2/50\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.7098 - acc: 0.5563 - val_loss: 0.6853 - val_acc: 0.5810\n",
      "Epoch 3/50\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.7047 - acc: 0.5610 - val_loss: 0.6929 - val_acc: 0.5990\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 4/50\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.6916 - acc: 0.5722 - val_loss: 0.6828 - val_acc: 0.6000\n",
      "Epoch 5/50\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.6869 - acc: 0.5655 - val_loss: 0.6806 - val_acc: 0.6000\n",
      "Epoch 6/50\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.6859 - acc: 0.5710 - val_loss: 0.6797 - val_acc: 0.6020\n",
      "Epoch 7/50\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.6886 - acc: 0.5745 - val_loss: 0.6790 - val_acc: 0.6030\n",
      "Epoch 8/50\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.6846 - acc: 0.5767 - val_loss: 0.6791 - val_acc: 0.6030\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 9/50\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.6814 - acc: 0.5825 - val_loss: 0.6793 - val_acc: 0.6030\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00009: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f70d30fdc18>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1, restore_best_weights=True)\n",
    "reducel = ReduceLROnPlateau(monitor='val_loss', patience=1, verbose=1, factor=0.1)\n",
    "\n",
    "model.fit(x = X_train, y = Y_train, epochs = 50, batch_size = 32, validation_data=(X_test,Y_test),callbacks=[reducel, earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 433us/step\n",
      "\n",
      "Loss = 0.6790213828086853\n",
      "Test Accuracy = 0.603\n"
     ]
    }
   ],
   "source": [
    "preds = model.evaluate(x = X_test, y = Y_test)\n",
    "print()\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.60629666 0.39370334]\n",
      "[0.58662933 0.41337067]\n",
      "[0.6395826  0.36041746]\n",
      "[0.6382677  0.36173227]\n",
      "[0.5860308  0.41396922]\n",
      "[0.5733769 0.4266231]\n",
      "[0.59028447 0.40971553]\n",
      "[0.5977267  0.40227324]\n",
      "[0.5842048  0.41579524]\n",
      "[0.5777173  0.42228267]\n",
      "[0.6226528 0.3773472]\n",
      "[0.63128674 0.36871323]\n",
      "[0.58917123 0.41082874]\n",
      "[0.6441915  0.35580847]\n",
      "[0.5798311  0.42016885]\n",
      "[0.6454003  0.35459968]\n",
      "[0.58825976 0.41174024]\n",
      "[0.61114013 0.38885987]\n",
      "[0.57965535 0.42034465]\n",
      "[0.61013377 0.38986623]\n",
      "[0.6490514  0.35094854]\n",
      "[0.6396956 0.3603044]\n",
      "[0.5911095  0.40889052]\n",
      "[0.6543318  0.34566817]\n",
      "[0.5995446  0.40045547]\n",
      "[0.62875235 0.37124768]\n",
      "[0.58687663 0.41312337]\n",
      "[0.60828644 0.39171353]\n",
      "[0.5927745  0.40722546]\n",
      "[0.5852055  0.41479447]\n",
      "[0.5805594  0.41944066]\n",
      "[0.60588443 0.3941156 ]\n",
      "[0.52390116 0.4760988 ]\n",
      "[0.5261471  0.47385287]\n",
      "[0.65912455 0.34087548]\n",
      "[0.5885878 0.4114122]\n",
      "[0.6469986  0.35300142]\n",
      "[0.6501294 0.3498706]\n",
      "[0.626263   0.37373692]\n",
      "[0.64040583 0.35959414]\n",
      "[0.60599995 0.39400002]\n",
      "[0.5971629 0.4028371]\n",
      "[0.56346875 0.43653128]\n",
      "[0.64921415 0.35078582]\n",
      "[0.6013665 0.3986335]\n",
      "[0.5578737 0.4421263]\n",
      "[0.5649396  0.43506035]\n",
      "[0.59556    0.40444002]\n",
      "[0.5235987 0.4764013]\n",
      "[0.5472855  0.45271447]\n",
      "[0.61598796 0.38401207]\n",
      "[0.58382285 0.41617715]\n",
      "[0.5726114  0.42738858]\n",
      "[0.5939362  0.40606382]\n",
      "[0.6029733  0.39702675]\n",
      "[0.5709658 0.4290342]\n",
      "[0.5283131  0.47168687]\n",
      "[0.6567826  0.34321734]\n",
      "[0.64189386 0.3581061 ]\n",
      "[0.5918586  0.40814137]\n",
      "[0.6465673  0.35343274]\n",
      "[0.57136476 0.42863527]\n",
      "[0.58262235 0.41737768]\n",
      "[0.5956857  0.40431434]\n",
      "[0.60370904 0.39629102]\n",
      "[0.6004767 0.3995233]\n",
      "[0.5848608  0.41513917]\n",
      "[0.63404655 0.3659535 ]\n",
      "[0.6116416  0.38835844]\n",
      "[0.635068   0.36493197]\n",
      "[0.592041   0.40795898]\n",
      "[0.5777563  0.42224368]\n",
      "[0.6052557  0.39474425]\n",
      "[0.5745256  0.42547444]\n",
      "[0.6299698 0.3700302]\n",
      "[0.63083386 0.36916614]\n",
      "[0.64307326 0.35692677]\n",
      "[0.65290195 0.34709808]\n",
      "[0.65156424 0.34843582]\n",
      "[0.55536634 0.44463366]\n",
      "[0.5981551  0.40184498]\n",
      "[0.603381   0.39661905]\n",
      "[0.59675884 0.4032412 ]\n",
      "[0.6356422  0.36435777]\n",
      "[0.5830637  0.41693622]\n",
      "[0.55720264 0.44279736]\n",
      "[0.64565206 0.35434794]\n",
      "[0.595411 0.404589]\n",
      "[0.6475747  0.35242522]\n",
      "[0.64467955 0.35532042]\n",
      "[0.6596739 0.340326 ]\n",
      "[0.612984 0.387016]\n",
      "[0.55979884 0.44020116]\n",
      "[0.56564367 0.43435633]\n",
      "[0.55697227 0.4430278 ]\n",
      "[0.62492794 0.375072  ]\n",
      "[0.644088 0.355912]\n",
      "[0.6238569  0.37614307]\n",
      "[0.63967234 0.36032763]\n",
      "[0.63215405 0.36784592]\n",
      "[0.59442073 0.40557924]\n",
      "[0.5798518  0.42014816]\n",
      "[0.56679827 0.43320173]\n",
      "[0.5204777 0.4795223]\n",
      "[0.60194856 0.3980514 ]\n",
      "[0.604636 0.395364]\n",
      "[0.6063345 0.3936655]\n",
      "[0.59754914 0.40245086]\n",
      "[0.55279446 0.44720557]\n",
      "[0.65693843 0.34306154]\n",
      "[0.5773293 0.4226707]\n",
      "[0.58593625 0.41406378]\n",
      "[0.6292054 0.3707946]\n",
      "[0.6242292  0.37577084]\n",
      "[0.5949466  0.40505338]\n",
      "[0.596752   0.40324804]\n",
      "[0.66718566 0.3328143 ]\n",
      "[0.56317747 0.43682256]\n",
      "[0.59685165 0.40314835]\n",
      "[0.5881587  0.41184124]\n",
      "[0.6249005  0.37509954]\n",
      "[0.5796497  0.42035034]\n",
      "[0.59396476 0.4060352 ]\n",
      "[0.6809703  0.31902972]\n",
      "[0.58071685 0.41928315]\n",
      "[0.60099846 0.3990015 ]\n",
      "[0.6142649  0.38573512]\n",
      "[0.6057468  0.39425322]\n",
      "[0.62154216 0.3784578 ]\n",
      "[0.53614783 0.46385217]\n",
      "[0.5925633  0.40743676]\n",
      "[0.64764845 0.35235152]\n",
      "[0.6005734  0.39942652]\n",
      "[0.5858524 0.4141476]\n",
      "[0.56499994 0.43500012]\n",
      "[0.6373226 0.3626774]\n",
      "[0.58001024 0.41998976]\n",
      "[0.60133165 0.39866835]\n",
      "[0.580014   0.41998604]\n",
      "[0.6109261  0.38907394]\n",
      "[0.5927872  0.40721276]\n",
      "[0.61858976 0.38141027]\n",
      "[0.57096237 0.42903757]\n",
      "[0.58024544 0.4197546 ]\n",
      "[0.5771739  0.42282614]\n",
      "[0.6610119  0.33898816]\n",
      "[0.60133195 0.3986681 ]\n",
      "[0.58290136 0.41709864]\n",
      "[0.6233613  0.37663865]\n",
      "[0.60371155 0.39628842]\n",
      "[0.5621207  0.43787926]\n",
      "[0.55798334 0.4420167 ]\n",
      "[0.60957986 0.39042017]\n",
      "[0.58418745 0.41581258]\n",
      "[0.5882248  0.41177523]\n",
      "[0.6464998  0.35350013]\n",
      "[0.6146082  0.38539174]\n",
      "[0.55078876 0.4492112 ]\n",
      "[0.67481935 0.32518062]\n",
      "[0.583226   0.41677397]\n",
      "[0.64518577 0.35481423]\n",
      "[0.6147453  0.38525468]\n",
      "[0.53992206 0.46007797]\n",
      "[0.5856518  0.41434816]\n",
      "[0.59032565 0.40967438]\n",
      "[0.56044394 0.4395561 ]\n",
      "[0.6198918  0.38010818]\n",
      "[0.57172316 0.42827684]\n",
      "[0.5458119  0.45418808]\n",
      "[0.59598184 0.40401816]\n",
      "[0.6195402  0.38045982]\n",
      "[0.58055264 0.41944733]\n",
      "[0.56334823 0.43665174]\n",
      "[0.6038068  0.39619324]\n",
      "[0.57768214 0.42231786]\n",
      "[0.53654313 0.46345684]\n",
      "[0.5893985  0.41060147]\n",
      "[0.5401104  0.45988956]\n",
      "[0.5642243  0.43577576]\n",
      "[0.59754395 0.40245607]\n",
      "[0.574141 0.425859]\n",
      "[0.58770305 0.4122969 ]\n",
      "[0.5915332  0.40846685]\n",
      "[0.653941   0.34605908]\n",
      "[0.5275555  0.47244447]\n",
      "[0.6062167  0.39378327]\n",
      "[0.54020375 0.45979628]\n",
      "[0.6303274  0.36967254]\n",
      "[0.6583478  0.34165227]\n",
      "[0.6528747  0.34712535]\n",
      "[0.6631061  0.33689395]\n",
      "[0.5970101  0.40298986]\n",
      "[0.6045717 0.3954283]\n",
      "[0.5843426 0.4156574]\n",
      "[0.60364443 0.3963555 ]\n",
      "[0.58611387 0.4138862 ]\n",
      "[0.6411252 0.3588748]\n",
      "[0.63701886 0.36298114]\n",
      "[0.6213628 0.3786372]\n",
      "[0.4742426 0.5257574]\n",
      "[0.69211894 0.3078811 ]\n",
      "[0.62350994 0.37649006]\n",
      "[0.5989787 0.4010213]\n",
      "[0.5944469 0.4055531]\n",
      "[0.59010464 0.4098954 ]\n",
      "[0.65832984 0.34167016]\n",
      "[0.6125209  0.38747913]\n",
      "[0.5888299  0.41117013]\n",
      "[0.60549045 0.39450955]\n",
      "[0.6303687  0.36963132]\n",
      "[0.6286895  0.37131047]\n",
      "[0.6173114 0.3826886]\n",
      "[0.6195472  0.38045287]\n",
      "[0.56597525 0.43402478]\n",
      "[0.5719889  0.42801112]\n",
      "[0.5899202 0.4100798]\n",
      "[0.5993296  0.40067035]\n",
      "[0.53160304 0.46839696]\n",
      "[0.5865769  0.41342315]\n",
      "[0.60778534 0.39221463]\n",
      "[0.6209502 0.3790498]\n",
      "[0.63612515 0.36387482]\n",
      "[0.6164229  0.38357717]\n",
      "[0.58148664 0.41851336]\n",
      "[0.62415093 0.37584907]\n",
      "[0.55899596 0.44100404]\n",
      "[0.6065692  0.39343086]\n",
      "[0.57449234 0.42550766]\n",
      "[0.59847265 0.40152737]\n",
      "[0.6439532  0.35604677]\n",
      "[0.58335894 0.41664103]\n",
      "[0.6302402  0.36975983]\n",
      "[0.6358014  0.36419868]\n",
      "[0.6455642  0.35443577]\n",
      "[0.63508326 0.36491668]\n",
      "[0.517071   0.48292905]\n",
      "[0.5903864  0.40961364]\n",
      "[0.6381349 0.3618651]\n",
      "[0.6377582  0.36224177]\n",
      "[0.6467676  0.35323235]\n",
      "[0.6607836 0.3392164]\n",
      "[0.64250064 0.3574994 ]\n",
      "[0.6089029  0.39109713]\n",
      "[0.6016087 0.3983913]\n",
      "[0.6397469 0.3602531]\n",
      "[0.61942726 0.38057274]\n",
      "[0.5626418  0.43735817]\n",
      "[0.6448422  0.35515782]\n",
      "[0.60965925 0.3903407 ]\n",
      "[0.60194397 0.39805606]\n",
      "[0.59647477 0.40352523]\n",
      "[0.64658433 0.3534157 ]\n",
      "[0.5545554 0.4454446]\n",
      "[0.67561024 0.3243898 ]\n",
      "[0.5638542  0.43614578]\n",
      "[0.56908697 0.4309131 ]\n",
      "[0.6569576  0.34304237]\n",
      "[0.5955865 0.4044135]\n",
      "[0.64752257 0.3524774 ]\n",
      "[0.665723   0.33427703]\n",
      "[0.5530863  0.44691372]\n",
      "[0.5928882 0.4071118]\n",
      "[0.6567741  0.34322584]\n",
      "[0.63419956 0.36580047]\n",
      "[0.58084154 0.41915846]\n",
      "[0.63182193 0.36817807]\n",
      "[0.6120133  0.38798675]\n",
      "[0.52539384 0.4746062 ]\n",
      "[0.5944951 0.4055049]\n",
      "[0.51664746 0.4833526 ]\n",
      "[0.59272116 0.4072788 ]\n",
      "[0.638884   0.36111602]\n",
      "[0.5673542 0.4326458]\n",
      "[0.5806756  0.41932437]\n",
      "[0.63223004 0.36776993]\n",
      "[0.5289903  0.47100967]\n",
      "[0.6548825 0.3451175]\n",
      "[0.58734107 0.41265893]\n",
      "[0.62798077 0.37201923]\n",
      "[0.61735094 0.3826491 ]\n",
      "[0.5848823 0.4151177]\n",
      "[0.6270507  0.37294933]\n",
      "[0.6015115 0.3984885]\n",
      "[0.65591484 0.3440851 ]\n",
      "[0.6148782  0.38512188]\n",
      "[0.6304696  0.36953032]\n",
      "[0.6611817  0.33881825]\n",
      "[0.61417884 0.38582116]\n",
      "[0.60639393 0.39360607]\n",
      "[0.5853875  0.41461247]\n",
      "[0.58965224 0.41034776]\n",
      "[0.5494289  0.45057112]\n",
      "[0.6053723  0.39462772]\n",
      "[0.60241055 0.39758945]\n",
      "[0.62759614 0.37240383]\n",
      "[0.55779433 0.4422056 ]\n",
      "[0.57039857 0.42960143]\n",
      "[0.56243014 0.43756986]\n",
      "[0.6510186 0.3489814]\n",
      "[0.48810738 0.5118927 ]\n",
      "[0.6003344  0.39966562]\n",
      "[0.6536055  0.34639448]\n",
      "[0.5903823  0.40961775]\n",
      "[0.6537613 0.3462387]\n",
      "[0.6342003  0.36579967]\n",
      "[0.58923876 0.41076127]\n",
      "[0.57447493 0.42552507]\n",
      "[0.5856703  0.41432977]\n",
      "[0.5853866 0.4146134]\n",
      "[0.6183459  0.38165408]\n",
      "[0.58681273 0.4131872 ]\n",
      "[0.57907015 0.42092982]\n",
      "[0.6224231  0.37757692]\n",
      "[0.5342049 0.4657951]\n",
      "[0.5635163  0.43648362]\n",
      "[0.60569257 0.3943075 ]\n",
      "[0.6540446 0.3459554]\n",
      "[0.5828536  0.41714638]\n",
      "[0.5178831  0.48211688]\n",
      "[0.6298818  0.37011823]\n",
      "[0.5797348  0.42026523]\n",
      "[0.55387026 0.44612968]\n",
      "[0.64411724 0.35588273]\n",
      "[0.59505105 0.40494895]\n",
      "[0.5707913  0.42920867]\n",
      "[0.62369776 0.3763022 ]\n",
      "[0.63199645 0.36800352]\n",
      "[0.6119048  0.38809523]\n",
      "[0.63941497 0.36058497]\n",
      "[0.5674323 0.4325677]\n",
      "[0.6073086 0.3926914]\n",
      "[0.6288841  0.37111595]\n",
      "[0.61617863 0.38382137]\n",
      "[0.5688753  0.43112466]\n",
      "[0.5821008  0.41789925]\n",
      "[0.59639907 0.40360093]\n",
      "[0.55571336 0.44428658]\n",
      "[0.6490731  0.35092685]\n",
      "[0.5796564  0.42034358]\n",
      "[0.6486801 0.3513199]\n",
      "[0.6116918 0.3883082]\n",
      "[0.6047107 0.3952893]\n",
      "[0.6281901  0.37180996]\n",
      "[0.5969626  0.40303743]\n",
      "[0.60058504 0.399415  ]\n",
      "[0.6061564  0.39384362]\n",
      "[0.5741539  0.42584613]\n",
      "[0.63458806 0.36541194]\n",
      "[0.5792648  0.42073518]\n",
      "[0.56503266 0.43496737]\n",
      "[0.6308686  0.36913142]\n",
      "[0.55041754 0.4495824 ]\n",
      "[0.57032245 0.42967755]\n",
      "[0.591245   0.40875503]\n",
      "[0.57368255 0.42631745]\n",
      "[0.5996791  0.40032083]\n",
      "[0.6173175  0.38268247]\n",
      "[0.5847953 0.4152047]\n",
      "[0.60328215 0.3967179 ]\n",
      "[0.58967435 0.41032562]\n",
      "[0.5388965 0.4611035]\n",
      "[0.61689913 0.3831009 ]\n",
      "[0.5855756  0.41442445]\n",
      "[0.60700864 0.39299133]\n",
      "[0.5663239  0.43367612]\n",
      "[0.5994489 0.4005511]\n",
      "[0.58261067 0.41738936]\n",
      "[0.5827017  0.41729832]\n",
      "[0.6010738  0.39892617]\n",
      "[0.508793 0.491207]\n",
      "[0.62998825 0.37001175]\n",
      "[0.53777134 0.4622287 ]\n",
      "[0.5566113 0.4433887]\n",
      "[0.6413827  0.35861734]\n",
      "[0.55727816 0.44272187]\n",
      "[0.5869198  0.41308022]\n",
      "[0.6132295 0.3867705]\n",
      "[0.61935365 0.38064635]\n",
      "[0.6041637  0.39583626]\n",
      "[0.604317   0.39568302]\n",
      "[0.6558821 0.3441179]\n",
      "[0.5991214  0.40087864]\n",
      "[0.5811387  0.41886136]\n",
      "[0.58831984 0.4116802 ]\n",
      "[0.6346247  0.36537522]\n",
      "[0.6275423 0.3724577]\n",
      "[0.58578837 0.4142116 ]\n",
      "[0.55872226 0.44127777]\n",
      "[0.64113516 0.35886484]\n",
      "[0.63495976 0.36504027]\n",
      "[0.58786577 0.4121342 ]\n",
      "[0.65245223 0.34754774]\n",
      "[0.64273864 0.3572614 ]\n",
      "[0.6067035  0.39329648]\n",
      "[0.5936687  0.40633124]\n",
      "[0.54709625 0.45290372]\n",
      "[0.6276542  0.37234586]\n",
      "[0.63964367 0.36035633]\n",
      "[0.5670441  0.43295592]\n",
      "[0.63666165 0.36333838]\n",
      "[0.58782977 0.4121703 ]\n",
      "[0.5813604 0.4186396]\n",
      "[0.64133704 0.35866293]\n",
      "[0.5026226  0.49737746]\n",
      "[0.6107377  0.38926226]\n",
      "[0.65889686 0.34110317]\n",
      "[0.62299234 0.37700766]\n",
      "[0.6418161  0.35818392]\n",
      "[0.5853833  0.41461667]\n",
      "[0.66096324 0.33903676]\n",
      "[0.5955935  0.40440655]\n",
      "[0.63890064 0.3610994 ]\n",
      "[0.5470507  0.45294926]\n",
      "[0.5939623 0.4060377]\n",
      "[0.6449635  0.35503656]\n",
      "[0.5938901 0.40611  ]\n",
      "[0.6347029  0.36529705]\n",
      "[0.5955793  0.40442067]\n",
      "[0.63217545 0.3678245 ]\n",
      "[0.58502036 0.41497964]\n",
      "[0.5614725  0.43852752]\n",
      "[0.5517855 0.4482145]\n",
      "[0.58231    0.41768995]\n",
      "[0.6045889  0.39541107]\n",
      "[0.6151129  0.38488716]\n",
      "[0.62523675 0.37476322]\n",
      "[0.6006183 0.3993817]\n",
      "[0.61397547 0.38602456]\n",
      "[0.6344155 0.3655845]\n",
      "[0.6155594  0.38444057]\n",
      "[0.58702093 0.412979  ]\n",
      "[0.59958893 0.40041107]\n",
      "[0.6506575  0.34934253]\n",
      "[0.6542949  0.34570512]\n",
      "[0.5810607 0.4189393]\n",
      "[0.56166756 0.43833247]\n",
      "[0.60496324 0.3950367 ]\n",
      "[0.57453966 0.42546034]\n",
      "[0.6193741  0.38062593]\n",
      "[0.62727755 0.37272248]\n",
      "[0.6052151  0.39478487]\n",
      "[0.5940585 0.4059415]\n",
      "[0.61656415 0.38343585]\n",
      "[0.609488   0.39051196]\n",
      "[0.57363874 0.4263613 ]\n",
      "[0.58875686 0.4112431 ]\n",
      "[0.6128095 0.3871905]\n",
      "[0.59879285 0.40120718]\n",
      "[0.6371937  0.36280635]\n",
      "[0.61801344 0.38198653]\n",
      "[0.63678765 0.36321235]\n",
      "[0.59890985 0.40109015]\n",
      "[0.54447836 0.4555216 ]\n",
      "[0.5962731  0.40372688]\n",
      "[0.6203511  0.37964895]\n",
      "[0.58057547 0.41942453]\n",
      "[0.62741965 0.37258032]\n",
      "[0.59205145 0.40794852]\n",
      "[0.6266738  0.37332615]\n",
      "[0.56753635 0.43246368]\n",
      "[0.614343   0.38565695]\n",
      "[0.6185053 0.3814947]\n",
      "[0.6487962 0.3512038]\n",
      "[0.5679702  0.43202972]\n",
      "[0.6720274  0.32797256]\n",
      "[0.5873794 0.4126206]\n",
      "[0.62794864 0.37205136]\n",
      "[0.6414674 0.3585326]\n",
      "[0.6639756  0.33602434]\n",
      "[0.6454914  0.35450855]\n",
      "[0.6435521  0.35644785]\n",
      "[0.6265553  0.37344468]\n",
      "[0.6072683  0.39273176]\n",
      "[0.5912917 0.4087083]\n",
      "[0.64997816 0.3500218 ]\n",
      "[0.5923442  0.40765584]\n",
      "[0.5496322 0.4503678]\n",
      "[0.5939726  0.40602735]\n",
      "[0.6008612 0.3991388]\n",
      "[0.63649356 0.36350647]\n",
      "[0.5754571  0.42454296]\n",
      "[0.58267397 0.41732606]\n",
      "[0.5662111  0.43378887]\n",
      "[0.57414055 0.42585942]\n",
      "[0.6000815  0.39991853]\n",
      "[0.5855421 0.4144579]\n",
      "[0.6024932  0.39750677]\n",
      "[0.6219138  0.37808618]\n",
      "[0.60072285 0.39927715]\n",
      "[0.57199323 0.4280068 ]\n",
      "[0.6474898  0.35251018]\n",
      "[0.6005623 0.3994377]\n",
      "[0.6192036 0.3807964]\n",
      "[0.60388297 0.39611706]\n",
      "[0.55835164 0.44164833]\n",
      "[0.5720292  0.42797083]\n",
      "[0.6466571  0.35334292]\n",
      "[0.55748475 0.44251522]\n",
      "[0.62954843 0.37045154]\n",
      "[0.6082483  0.39175168]\n",
      "[0.5715256  0.42847437]\n",
      "[0.5889262  0.41107386]\n",
      "[0.63170594 0.36829406]\n",
      "[0.5978887  0.40211126]\n",
      "[0.6029509  0.39704916]\n",
      "[0.5730354  0.42696464]\n",
      "[0.59272695 0.40727305]\n",
      "[0.5981365  0.40186352]\n",
      "[0.57528526 0.42471477]\n",
      "[0.589027   0.41097304]\n",
      "[0.60233253 0.39766744]\n",
      "[0.61217165 0.38782838]\n",
      "[0.61645275 0.38354728]\n",
      "[0.64891875 0.35108122]\n",
      "[0.64966035 0.35033968]\n",
      "[0.57805663 0.42194337]\n",
      "[0.58411777 0.4158822 ]\n",
      "[0.5565288  0.44347113]\n",
      "[0.6674514  0.33254865]\n",
      "[0.6001278 0.3998722]\n",
      "[0.59020627 0.40979373]\n",
      "[0.5902265  0.40977353]\n",
      "[0.57431203 0.425688  ]\n",
      "[0.60349107 0.39650893]\n",
      "[0.63223964 0.36776036]\n",
      "[0.56119734 0.43880266]\n",
      "[0.5945849  0.40541512]\n",
      "[0.566481   0.43351898]\n",
      "[0.61602867 0.3839713 ]\n",
      "[0.6528723  0.34712768]\n",
      "[0.6667765  0.33322355]\n",
      "[0.587522   0.41247803]\n",
      "[0.59395725 0.4060427 ]\n",
      "[0.58321923 0.4167807 ]\n",
      "[0.63154083 0.3684592 ]\n",
      "[0.60136676 0.39863324]\n",
      "[0.5936635  0.40633646]\n",
      "[0.58003634 0.41996366]\n",
      "[0.5970011  0.40299886]\n",
      "[0.5873571  0.41264293]\n",
      "[0.58712655 0.41287342]\n",
      "[0.58938175 0.41061828]\n",
      "[0.5910437  0.40895623]\n",
      "[0.63181245 0.36818758]\n",
      "[0.6668999 0.3331001]\n",
      "[0.59051377 0.4094862 ]\n",
      "[0.5276444  0.47235557]\n",
      "[0.6040762  0.39592385]\n",
      "[0.6029584  0.39704162]\n",
      "[0.6360849  0.36391512]\n",
      "[0.6018735  0.39812648]\n",
      "[0.65012455 0.34987548]\n",
      "[0.570848   0.42915204]\n",
      "[0.5747692  0.42523086]\n",
      "[0.66355586 0.3364442 ]\n",
      "[0.5932006 0.4067994]\n",
      "[0.6035709  0.39642915]\n",
      "[0.57021874 0.4297812 ]\n",
      "[0.5442172 0.4557828]\n",
      "[0.5955202  0.40447977]\n",
      "[0.56399524 0.43600476]\n",
      "[0.6311516 0.3688484]\n",
      "[0.60093904 0.3990609 ]\n",
      "[0.5448028  0.45519722]\n",
      "[0.6450369  0.35496306]\n",
      "[0.60844576 0.39155427]\n",
      "[0.6482591  0.35174087]\n",
      "[0.62788945 0.3721106 ]\n",
      "[0.5547711  0.44522884]\n",
      "[0.60838 0.39162]\n",
      "[0.59867406 0.4013259 ]\n",
      "[0.5919965  0.40800348]\n",
      "[0.62614214 0.37385786]\n",
      "[0.506382 0.493618]\n",
      "[0.5918048  0.40819526]\n",
      "[0.6430074  0.35699257]\n",
      "[0.58196026 0.41803977]\n",
      "[0.65423506 0.34576494]\n",
      "[0.53783196 0.462168  ]\n",
      "[0.64772344 0.35227656]\n",
      "[0.62534237 0.37465766]\n",
      "[0.6501944  0.34980556]\n",
      "[0.6262758  0.37372425]\n",
      "[0.5791929  0.42080712]\n",
      "[0.594613 0.405387]\n",
      "[0.56030744 0.43969253]\n",
      "[0.57609355 0.42390645]\n",
      "[0.5727417 0.4272583]\n",
      "[0.63190776 0.36809224]\n",
      "[0.6337767  0.36622322]\n",
      "[0.64206785 0.35793215]\n",
      "[0.6378997 0.3621003]\n",
      "[0.65932053 0.34067944]\n",
      "[0.5278377  0.47216234]\n",
      "[0.6245933  0.37540668]\n",
      "[0.5401816  0.45981848]\n",
      "[0.62136686 0.3786331 ]\n",
      "[0.63897264 0.3610274 ]\n",
      "[0.5908715 0.4091285]\n",
      "[0.5502868  0.44971317]\n",
      "[0.5931327  0.40686736]\n",
      "[0.5965413  0.40345868]\n",
      "[0.5944403  0.40555972]\n",
      "[0.5436985  0.45630145]\n",
      "[0.59962964 0.40037033]\n",
      "[0.59989864 0.40010133]\n",
      "[0.57175034 0.42824966]\n",
      "[0.6099932  0.39000678]\n",
      "[0.61485136 0.38514864]\n",
      "[0.6583151  0.34168485]\n",
      "[0.62136316 0.37863678]\n",
      "[0.5800075  0.41999254]\n",
      "[0.612882   0.38711804]\n",
      "[0.6027051 0.3972949]\n",
      "[0.5867369  0.41326314]\n",
      "[0.5892264  0.41077358]\n",
      "[0.56875896 0.43124107]\n",
      "[0.58922297 0.410777  ]\n",
      "[0.6556181  0.34438193]\n",
      "[0.63677007 0.36322987]\n",
      "[0.62986964 0.3701304 ]\n",
      "[0.61342025 0.38657972]\n",
      "[0.534674   0.46532598]\n",
      "[0.6652711  0.33472896]\n",
      "[0.5939687 0.4060313]\n",
      "[0.58303887 0.41696116]\n",
      "[0.6260718 0.3739282]\n",
      "[0.6009271 0.3990729]\n",
      "[0.6024597  0.39754024]\n",
      "[0.5673984 0.4326016]\n",
      "[0.6575795  0.34242052]\n",
      "[0.5931812  0.40681878]\n",
      "[0.6197855 0.3802145]\n",
      "[0.64649934 0.35350072]\n",
      "[0.5648451  0.43515489]\n",
      "[0.5442967 0.4557033]\n",
      "[0.63416034 0.36583963]\n",
      "[0.64095825 0.35904178]\n",
      "[0.63272035 0.36727962]\n",
      "[0.5755022  0.42449772]\n",
      "[0.5547008  0.44529918]\n",
      "[0.55258125 0.4474187 ]\n",
      "[0.63888854 0.3611114 ]\n",
      "[0.5638023 0.4361977]\n",
      "[0.60145414 0.39854592]\n",
      "[0.56666774 0.43333223]\n",
      "[0.57646155 0.42353845]\n",
      "[0.6321415 0.3678586]\n",
      "[0.65228945 0.34771058]\n",
      "[0.6047944  0.39520568]\n",
      "[0.6444632  0.35553682]\n",
      "[0.5946062  0.40539384]\n",
      "[0.59005994 0.40994003]\n",
      "[0.6460594  0.35394058]\n",
      "[0.5944538 0.4055462]\n",
      "[0.6024405  0.39755946]\n",
      "[0.5630508  0.43694922]\n",
      "[0.63668525 0.3633148 ]\n",
      "[0.5506284  0.44937155]\n",
      "[0.577376   0.42262396]\n",
      "[0.6002872 0.3997128]\n",
      "[0.62362224 0.37637773]\n",
      "[0.5974013 0.4025987]\n",
      "[0.63349557 0.3665044 ]\n",
      "[0.5656759  0.43432415]\n",
      "[0.6391859  0.36081412]\n",
      "[0.612792   0.38720798]\n",
      "[0.635739 0.364261]\n",
      "[0.5761922  0.42380777]\n",
      "[0.63520294 0.36479712]\n",
      "[0.5783966  0.42160332]\n",
      "[0.5823381  0.41766196]\n",
      "[0.61791474 0.38208523]\n",
      "[0.6106701 0.3893299]\n",
      "[0.6598194  0.34018055]\n",
      "[0.6602476  0.33975238]\n",
      "[0.5902126  0.40978736]\n",
      "[0.66330945 0.33669055]\n",
      "[0.6300937 0.3699063]\n",
      "[0.5809678 0.4190322]\n",
      "[0.5673607 0.4326393]\n",
      "[0.5998753  0.40012476]\n",
      "[0.6165819  0.38341805]\n",
      "[0.624963   0.37503698]\n",
      "[0.554981 0.445019]\n",
      "[0.6484484 0.3515516]\n",
      "[0.59030294 0.40969706]\n",
      "[0.6105731 0.3894269]\n",
      "[0.6390874  0.36091262]\n",
      "[0.54118097 0.458819  ]\n",
      "[0.61535394 0.3846461 ]\n",
      "[0.6007213 0.3992787]\n",
      "[0.56765467 0.4323453 ]\n",
      "[0.61155796 0.38844204]\n",
      "[0.6103512  0.38964882]\n",
      "[0.5718835 0.4281165]\n",
      "[0.63428515 0.36571488]\n",
      "[0.5835516  0.41644835]\n",
      "[0.5722743 0.4277257]\n",
      "[0.63518256 0.36481747]\n",
      "[0.6115152  0.38848478]\n",
      "[0.6164397  0.38356033]\n",
      "[0.65276474 0.34723526]\n",
      "[0.57492673 0.4250732 ]\n",
      "[0.55565655 0.44434348]\n",
      "[0.63794374 0.36205631]\n",
      "[0.63336384 0.3666362 ]\n",
      "[0.55640864 0.4435914 ]\n",
      "[0.6042001  0.39579988]\n",
      "[0.569838 0.430162]\n",
      "[0.567989   0.43201095]\n",
      "[0.6251371  0.37486294]\n",
      "[0.58900875 0.41099125]\n",
      "[0.64374816 0.3562518 ]\n",
      "[0.57130367 0.42869636]\n",
      "[0.6468786  0.35312143]\n",
      "[0.5628867  0.43711334]\n",
      "[0.63861835 0.3613817 ]\n",
      "[0.58133024 0.41866973]\n",
      "[0.57618445 0.42381558]\n",
      "[0.64458597 0.355414  ]\n",
      "[0.54618233 0.45381767]\n",
      "[0.5760251  0.42397484]\n",
      "[0.61962175 0.38037825]\n",
      "[0.63269013 0.36730987]\n",
      "[0.6172718  0.38272822]\n",
      "[0.6461284  0.35387158]\n",
      "[0.6315921  0.36840785]\n",
      "[0.56811196 0.431888  ]\n",
      "[0.5239253  0.47607473]\n",
      "[0.6466585  0.35334146]\n",
      "[0.5453923  0.45460767]\n",
      "[0.54293996 0.45706004]\n",
      "[0.61054325 0.38945675]\n",
      "[0.5924428 0.4075572]\n",
      "[0.55791175 0.4420883 ]\n",
      "[0.58758223 0.41241774]\n",
      "[0.6035773 0.3964227]\n",
      "[0.6207963  0.37920365]\n",
      "[0.56848973 0.43151024]\n",
      "[0.49494392 0.5050561 ]\n",
      "[0.57450354 0.42549646]\n",
      "[0.60026485 0.39973512]\n",
      "[0.6352059  0.36479405]\n",
      "[0.6423718  0.35762826]\n",
      "[0.63954544 0.3604546 ]\n",
      "[0.6456055 0.3543945]\n",
      "[0.6497626  0.35023743]\n",
      "[0.56336 0.43664]\n",
      "[0.58676    0.41324008]\n",
      "[0.53661287 0.4633871 ]\n",
      "[0.60639083 0.39360914]\n",
      "[0.5937541 0.4062459]\n",
      "[0.63036203 0.369638  ]\n",
      "[0.61696255 0.38303748]\n",
      "[0.58792704 0.412073  ]\n",
      "[0.55308855 0.44691142]\n",
      "[0.60136807 0.3986319 ]\n",
      "[0.5906904  0.40930957]\n",
      "[0.46727028 0.5327297 ]\n",
      "[0.6329629  0.36703715]\n",
      "[0.53169686 0.4683031 ]\n",
      "[0.60007304 0.39992693]\n",
      "[0.5599797  0.44002032]\n",
      "[0.64488    0.35511997]\n",
      "[0.5899621  0.41003782]\n",
      "[0.60133964 0.39866036]\n",
      "[0.6079981  0.39200187]\n",
      "[0.6016188 0.3983812]\n",
      "[0.605462   0.39453802]\n",
      "[0.64305 0.35695]\n",
      "[0.60419106 0.3958089 ]\n",
      "[0.62582725 0.37417275]\n",
      "[0.5959046 0.4040954]\n",
      "[0.5126867 0.4873133]\n",
      "[0.60804075 0.39195928]\n",
      "[0.5319929  0.46800712]\n",
      "[0.54701173 0.4529882 ]\n",
      "[0.59511787 0.40488213]\n",
      "[0.5886556  0.41134438]\n",
      "[0.610351 0.389649]\n",
      "[0.62519866 0.37480134]\n",
      "[0.5739904  0.42600957]\n",
      "[0.6412596  0.35874045]\n",
      "[0.59759015 0.40240982]\n",
      "[0.5963138  0.40368629]\n",
      "[0.64978397 0.35021606]\n",
      "[0.6438473  0.35615265]\n",
      "[0.6552745  0.34472552]\n",
      "[0.6424571 0.3575429]\n",
      "[0.5733128  0.42668715]\n",
      "[0.63501406 0.36498597]\n",
      "[0.6453858 0.3546142]\n",
      "[0.6121344  0.38786563]\n",
      "[0.6023454 0.3976546]\n",
      "[0.6018511  0.39814886]\n",
      "[0.65811235 0.34188765]\n",
      "[0.6515167  0.34848332]\n",
      "[0.5638082 0.4361918]\n",
      "[0.6547712 0.3452288]\n",
      "[0.6136912  0.38630876]\n",
      "[0.65742856 0.34257144]\n",
      "[0.5899234  0.41007665]\n",
      "[0.59548694 0.404513  ]\n",
      "[0.563481   0.43651906]\n",
      "[0.53884465 0.46115538]\n",
      "[0.636492   0.36350805]\n",
      "[0.6296411 0.3703589]\n",
      "[0.6618327  0.33816734]\n",
      "[0.56048906 0.43951097]\n",
      "[0.5913512 0.4086488]\n",
      "[0.618769   0.38123095]\n",
      "[0.5926561  0.40734398]\n",
      "[0.63564485 0.36435512]\n",
      "[0.59603405 0.40396592]\n",
      "[0.6274491 0.3725509]\n",
      "[0.65858036 0.3414196 ]\n",
      "[0.59150845 0.40849158]\n",
      "[0.61540025 0.3845998 ]\n",
      "[0.5781512 0.4218487]\n",
      "[0.6062384  0.39376163]\n",
      "[0.56081074 0.4391893 ]\n",
      "[0.62665397 0.373346  ]\n",
      "[0.599613 0.400387]\n",
      "[0.63556147 0.36443853]\n",
      "[0.5726072  0.42739278]\n",
      "[0.5626448  0.43735522]\n",
      "[0.60189575 0.3981042 ]\n",
      "[0.61063963 0.38936034]\n",
      "[0.60351145 0.39648855]\n",
      "[0.5856741  0.41432586]\n",
      "[0.59291166 0.40708828]\n",
      "[0.6527801  0.34721994]\n",
      "[0.5817823 0.4182177]\n",
      "[0.6433093 0.3566907]\n",
      "[0.5425568  0.45744315]\n",
      "[0.602127   0.39787298]\n",
      "[0.6261199  0.37388012]\n",
      "[0.632431   0.36756903]\n",
      "[0.5619189 0.4380811]\n",
      "[0.5975216  0.40247846]\n",
      "[0.639773   0.36022696]\n",
      "[0.62173665 0.37826338]\n",
      "[0.52553445 0.47446552]\n",
      "[0.63577765 0.36422235]\n",
      "[0.5696967 0.4303033]\n",
      "[0.6058986  0.39410135]\n",
      "[0.6277477  0.37225223]\n",
      "[0.650069 0.349931]\n",
      "[0.6436213 0.3563787]\n",
      "[0.6470522  0.35294774]\n",
      "[0.5459285 0.4540715]\n",
      "[0.609259   0.39074096]\n",
      "[0.61801326 0.38198674]\n",
      "[0.59601897 0.4039811 ]\n",
      "[0.6465016 0.3534984]\n",
      "[0.61126465 0.38873535]\n",
      "[0.5537057  0.44629434]\n",
      "[0.5807743 0.4192257]\n",
      "[0.61561644 0.38438356]\n",
      "[0.61026746 0.38973254]\n",
      "[0.6262068  0.37379318]\n",
      "[0.64960873 0.35039124]\n",
      "[0.5221379  0.47786212]\n",
      "[0.589765 0.410235]\n",
      "[0.6193028  0.38069716]\n",
      "[0.5669592  0.43304086]\n",
      "[0.6025534 0.3974466]\n",
      "[0.6463837 0.3536163]\n",
      "[0.61899906 0.38100094]\n",
      "[0.61639416 0.38360584]\n",
      "[0.59568757 0.4043124 ]\n",
      "[0.5876713  0.41232875]\n",
      "[0.5957934  0.40420654]\n",
      "[0.6636878 0.3363122]\n",
      "[0.6113425 0.3886575]\n",
      "[0.61556935 0.38443065]\n",
      "[0.58384925 0.41615072]\n",
      "[0.57777596 0.42222407]\n",
      "[0.5712212 0.4287788]\n",
      "[0.56684786 0.4331521 ]\n",
      "[0.6205427  0.37945732]\n",
      "[0.581723   0.41827708]\n",
      "[0.5919532  0.40804678]\n",
      "[0.63499564 0.36500436]\n",
      "[0.57730305 0.422697  ]\n",
      "[0.6218397  0.37816024]\n",
      "[0.64327973 0.35672033]\n",
      "[0.6677386  0.33226138]\n",
      "[0.57879347 0.42120653]\n",
      "[0.63595307 0.36404687]\n",
      "[0.6208317  0.37916827]\n",
      "[0.5659627 0.4340374]\n",
      "[0.57055527 0.4294447 ]\n",
      "[0.61271447 0.38728556]\n",
      "[0.57796013 0.42203993]\n",
      "[0.6164377 0.3835623]\n",
      "[0.5658932  0.43410683]\n",
      "[0.63924813 0.3607519 ]\n",
      "[0.59095275 0.40904725]\n",
      "[0.59597635 0.40402365]\n",
      "[0.595082   0.40491804]\n",
      "[0.5938031  0.40619692]\n",
      "[0.6150354  0.38496464]\n",
      "[0.6209194 0.3790806]\n",
      "[0.59142774 0.40857226]\n",
      "[0.60773355 0.3922665 ]\n",
      "[0.61969286 0.38030714]\n",
      "[0.62406915 0.3759308 ]\n",
      "[0.58458245 0.41541755]\n",
      "[0.5918655  0.40813452]\n",
      "[0.5631235 0.4368765]\n",
      "[0.6076694  0.39233056]\n",
      "[0.57848066 0.4215193 ]\n",
      "[0.65413517 0.3458648 ]\n",
      "[0.5930605  0.40693957]\n",
      "[0.61951494 0.380485  ]\n",
      "[0.56434345 0.4356565 ]\n",
      "[0.67057246 0.32942757]\n",
      "[0.6873639  0.31263608]\n",
      "[0.5925671  0.40743297]\n",
      "[0.65517956 0.34482044]\n",
      "[0.5672797  0.43272033]\n",
      "[0.5398119 0.4601881]\n",
      "[0.55685616 0.44314387]\n",
      "[0.6099464  0.39005357]\n",
      "[0.5890955  0.41090453]\n",
      "[0.63507617 0.3649238 ]\n",
      "[0.61028403 0.38971597]\n",
      "[0.6584504  0.34154963]\n",
      "[0.66036254 0.3396375 ]\n",
      "[0.58206683 0.41793317]\n",
      "[0.6246735  0.37532654]\n",
      "[0.5636016  0.43639842]\n",
      "[0.62230814 0.3776918 ]\n",
      "[0.62347555 0.37652448]\n",
      "[0.6335219  0.36647806]\n",
      "[0.59796053 0.4020395 ]\n",
      "[0.5787826 0.4212174]\n",
      "[0.6460774  0.35392258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5783355  0.42166448]\n",
      "[0.5705706 0.4294294]\n",
      "[0.61666167 0.38333836]\n",
      "[0.5975467  0.40245327]\n",
      "[0.59602755 0.40397245]\n",
      "[0.5843053  0.41569474]\n",
      "[0.6142188  0.38578123]\n",
      "[0.63540965 0.36459032]\n",
      "[0.5990054  0.40099466]\n",
      "[0.5453496  0.45465037]\n",
      "[0.5937414  0.40625855]\n",
      "[0.59955674 0.40044323]\n",
      "[0.5803115  0.41968855]\n",
      "[0.64674497 0.35325503]\n",
      "[0.55091983 0.4490802 ]\n",
      "[0.63279855 0.36720145]\n",
      "[0.56628656 0.43371347]\n",
      "[0.59596306 0.40403694]\n",
      "[0.5898951  0.41010493]\n",
      "[0.59510833 0.4048916 ]\n",
      "[0.5835392 0.4164608]\n",
      "[0.6088137  0.39118633]\n",
      "[0.5548956  0.44510442]\n",
      "[0.63979036 0.36020964]\n",
      "[0.6531707 0.3468293]\n",
      "[0.6299602  0.37003985]\n",
      "[0.5804632  0.41953683]\n",
      "[0.6518505  0.34814954]\n",
      "[0.65456396 0.34543607]\n",
      "[0.60096383 0.39903614]\n",
      "[0.6478927  0.35210732]\n",
      "[0.58825374 0.4117462 ]\n",
      "[0.6330654  0.36693454]\n",
      "[0.5950046  0.40499544]\n",
      "[0.60088295 0.3991171 ]\n",
      "[0.5206546 0.4793454]\n",
      "[0.6289464  0.37105358]\n",
      "[0.6514885  0.34851152]\n",
      "[0.62679595 0.37320402]\n",
      "[0.6217928  0.37820724]\n",
      "[0.6374437 0.3625563]\n",
      "[0.64625037 0.35374966]\n",
      "[0.547451   0.45254898]\n",
      "[0.6342008  0.36579916]\n",
      "[0.58640856 0.41359144]\n",
      "[0.6548427 0.3451573]\n",
      "[0.60434705 0.39565298]\n",
      "[0.59274596 0.4072541 ]\n",
      "[0.6504855 0.3495145]\n",
      "[0.5765217 0.4234783]\n",
      "[0.58162546 0.4183746 ]\n",
      "[0.63486725 0.3651327 ]\n",
      "[0.6054778  0.39452216]\n",
      "[0.612956   0.38704404]\n",
      "[0.6642041  0.33579585]\n",
      "[0.61057997 0.38942   ]\n",
      "[0.6095857  0.39041424]\n",
      "[0.6027752 0.3972248]\n",
      "[0.5360277  0.46397224]\n",
      "[0.59683484 0.40316513]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "[print(y) for y in y_pred] \n",
    "#[print(y) for y in Y_test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4410042297035015"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "y_pred = model.predict(X_test)\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(Y_test[:, 1], y_pred[:,1])\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "auc_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='area = {:.3f}'.format(auc_keras))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model = keras.applications.vgg16.VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.engine.training.Model"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the type\n",
    "type(vgg16_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze some layers\n",
    "for layer in vgg16_model.layers[:20]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own input format \n",
    "X_input = Input(shape=(image_size,image_size,3),name = 'image_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vgg16_conv = vgg16_model(X_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the fully-connected layers \n",
    "X = Flatten(name='flatten')(output_vgg16_conv)\n",
    "X = Dense(2048, activation='relu', name='fc1')(X)\n",
    "X = Dense(2048, activation='relu', name='fc2')(X)\n",
    "X = Dense(2, activation='softmax', name='predictions')(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = Model(inputs=X_input, outputs=X,name='myModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 96, 96, 3)         0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 2048)              9439232   \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 28,354,370\n",
      "Trainable params: 13,639,682\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "my_model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "4000/4000 [==============================] - 6s 2ms/step - loss: 0.6877 - acc: 0.5640 - val_loss: 0.6744 - val_acc: 0.6080\n",
      "Epoch 2/50\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.6879 - acc: 0.5740 - val_loss: 0.6745 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 3/50\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.6839 - acc: 0.5750 - val_loss: 0.6746 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0bb3754940>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size = 32, epochs = 50, verbose = 1, validation_data=(X_test,Y_test),callbacks=[reducel, earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 3s 3ms/step\n",
      "Loss = 0.7675437154769897\n",
      "Test Accuracy = 0.421\n"
     ]
    }
   ],
   "source": [
    "preds = my_model.evaluate(x = X_test, y = Y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionV3_model = keras.applications.inception_v3.InceptionV3(weights='imagenet',include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.engine.training.Model"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inceptionV3_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all the other layers, let last few layers to be trainable\n",
    "for layer in inceptionV3_model.layers[:290]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own input format\n",
    "X_input = Input(shape=(image_size,image_size,3),name = 'image_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_inceptionV3_conv = inceptionV3_model(X_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras recommended application\n",
    "X = GlobalAveragePooling2D()(output_inceptionV3_conv)\n",
    "X = Dense(1024, activation='relu')(X)\n",
    "X = Dense(2, activation='softmax', name='predictions')(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = Model(inputs=X_input, outputs=X,name='myModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 96, 96, 3)         0         \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         multiple                  21802784  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 23,903,010\n",
      "Trainable params: 4,477,954\n",
      "Non-trainable params: 19,425,056\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(4000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(Y_test.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 0.6853 - acc: 0.5800 - val_loss: 0.6745 - val_acc: 0.6090\n",
      "Epoch 2/50\n",
      "4000/4000 [==============================] - 6s 1ms/step - loss: 0.6860 - acc: 0.5677 - val_loss: 0.6745 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "Epoch 3/50\n",
      "4000/4000 [==============================] - 6s 1ms/step - loss: 0.6854 - acc: 0.5733 - val_loss: 0.6745 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "Epoch 4/50\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.6845 - acc: 0.5847 - val_loss: 0.6744 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "Epoch 5/50\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.6826 - acc: 0.5763 - val_loss: 0.6745 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "Epoch 6/50\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.6837 - acc: 0.5692 - val_loss: 0.6744 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0b95fa09e8>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size = 32, epochs = 50, verbose = 1, validation_data=(X_test,Y_test),callbacks=[reducel, earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 5s 5ms/step\n",
      "Loss = 0.842498948097229\n",
      "Test Accuracy = 0.54\n"
     ]
    }
   ],
   "source": [
    "preds = my_model.evaluate(x = X_test, y = Y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "dropout_fc = 0.5\n",
    "\n",
    "conv_base = ResNet50(weights = 'imagenet', include_top = False, input_shape = (image_size,image_size,3))\n",
    "\n",
    "my_model = Sequential()\n",
    "\n",
    "my_model.add(conv_base)\n",
    "my_model.add(Flatten())\n",
    "my_model.add(Dense(256, use_bias=False))\n",
    "my_model.add(BatchNormalization())\n",
    "my_model.add(Activation(\"relu\"))\n",
    "my_model.add(Dropout(dropout_fc))\n",
    "my_model.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 3, 3, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               4718592   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 28,307,585\n",
      "Trainable params: 28,253,953\n",
      "Non-trainable params: 53,632\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.Trainable=True\n",
    "\n",
    "set_trainable=False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'res5a_branch2a':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "my_model.compile(optimizers.Adam(0.001), loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own input format \n",
    "X_input = Input(shape=(image_size,image_size,3),name = 'image_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/9\n",
      "4000/4000 [==============================] - 29s 7ms/step - loss: 0.7645 - acc: 0.5683 - val_loss: 0.8444 - val_acc: 0.6040\n",
      "Epoch 2/9\n",
      "4000/4000 [==============================] - 20s 5ms/step - loss: 0.7197 - acc: 0.5893 - val_loss: 0.6990 - val_acc: 0.6090\n",
      "Epoch 3/9\n",
      "4000/4000 [==============================] - 20s 5ms/step - loss: 0.7109 - acc: 0.6287 - val_loss: 0.7313 - val_acc: 0.6050\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 4/9\n",
      "4000/4000 [==============================] - 20s 5ms/step - loss: 0.5869 - acc: 0.7645 - val_loss: 0.6934 - val_acc: 0.5970\n",
      "Epoch 5/9\n",
      "4000/4000 [==============================] - 20s 5ms/step - loss: 0.4870 - acc: 0.8342 - val_loss: 0.6958 - val_acc: 0.5700\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 6/9\n",
      "4000/4000 [==============================] - 20s 5ms/step - loss: 0.3479 - acc: 0.9073 - val_loss: 0.7110 - val_acc: 0.5420\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f19bd5d2208>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1, restore_best_weights=True)\n",
    "reducel = ReduceLROnPlateau(monitor='val_loss', patience=1, verbose=1, factor=0.1)\n",
    "my_model.fit(x = X_train, y = Y_train, epochs = 9, batch_size = 32, validation_data=(X_test,Y_test), callbacks=[reducel, earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step\n",
      "Loss = 0.6934362449645997\n",
      "Test Accuracy = 0.597\n"
     ]
    }
   ],
   "source": [
    "preds = my_model.evaluate(x = X_test, y = Y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4212867]\n",
      "[0.39174595]\n",
      "[0.40390238]\n",
      "[0.44005755]\n",
      "[0.47084934]\n",
      "[0.46770692]\n",
      "[0.4108221]\n",
      "[0.42734474]\n",
      "[0.40130153]\n",
      "[0.3452026]\n",
      "[0.48712403]\n",
      "[0.4731412]\n",
      "[0.3202902]\n",
      "[0.44441465]\n",
      "[0.43910733]\n",
      "[0.34190598]\n",
      "[0.11323726]\n",
      "[0.41759244]\n",
      "[0.5709752]\n",
      "[0.39542255]\n",
      "[0.3655085]\n",
      "[0.47389412]\n",
      "[0.46512574]\n",
      "[0.35545385]\n",
      "[0.40793008]\n",
      "[0.40201706]\n",
      "[0.45595586]\n",
      "[0.35429484]\n",
      "[0.479266]\n",
      "[0.47018456]\n",
      "[0.40247732]\n",
      "[0.3798739]\n",
      "[0.5182965]\n",
      "[0.49371335]\n",
      "[0.4358311]\n",
      "[0.271375]\n",
      "[0.4353197]\n",
      "[0.45108572]\n",
      "[0.2896011]\n",
      "[0.4419379]\n",
      "[0.42984664]\n",
      "[0.3538571]\n",
      "[0.3932947]\n",
      "[0.3970685]\n",
      "[0.43353403]\n",
      "[0.38597813]\n",
      "[0.4783542]\n",
      "[0.42155045]\n",
      "[0.43813664]\n",
      "[0.5548589]\n",
      "[0.38224393]\n",
      "[0.4982891]\n",
      "[0.42245302]\n",
      "[0.5122288]\n",
      "[0.51347303]\n",
      "[0.43237334]\n",
      "[0.49999604]\n",
      "[0.3769383]\n",
      "[0.37840107]\n",
      "[0.43985358]\n",
      "[0.4072265]\n",
      "[0.44003513]\n",
      "[0.366679]\n",
      "[0.46471685]\n",
      "[0.57139415]\n",
      "[0.49822474]\n",
      "[0.5102014]\n",
      "[0.40480825]\n",
      "[0.39880988]\n",
      "[0.45250043]\n",
      "[0.46101874]\n",
      "[0.48507243]\n",
      "[0.38802013]\n",
      "[0.4830412]\n",
      "[0.3177153]\n",
      "[0.2808095]\n",
      "[0.30457067]\n",
      "[0.33799797]\n",
      "[0.31234336]\n",
      "[0.44497818]\n",
      "[0.23991367]\n",
      "[0.29868615]\n",
      "[0.4257724]\n",
      "[0.36841983]\n",
      "[0.32611725]\n",
      "[0.5195211]\n",
      "[0.42595208]\n",
      "[0.5227834]\n",
      "[0.37964112]\n",
      "[0.20396543]\n",
      "[0.41882053]\n",
      "[0.4090309]\n",
      "[0.3678906]\n",
      "[0.20851737]\n",
      "[0.44441584]\n",
      "[0.39868352]\n",
      "[0.3585383]\n",
      "[0.44453853]\n",
      "[0.3663515]\n",
      "[0.51532334]\n",
      "[0.3050239]\n",
      "[0.5281932]\n",
      "[0.33165082]\n",
      "[0.35122412]\n",
      "[0.35282475]\n",
      "[0.48726857]\n",
      "[0.47165284]\n",
      "[0.44381133]\n",
      "[0.46918207]\n",
      "[0.3588423]\n",
      "[0.0712266]\n",
      "[0.5738264]\n",
      "[0.4017626]\n",
      "[0.40386075]\n",
      "[0.47190514]\n",
      "[0.4176674]\n",
      "[0.32873195]\n",
      "[0.45011276]\n",
      "[0.45279276]\n",
      "[0.41702557]\n",
      "[0.31604964]\n",
      "[0.48098123]\n",
      "[0.4416851]\n",
      "[0.39959738]\n",
      "[0.42307258]\n",
      "[0.40781114]\n",
      "[0.07267484]\n",
      "[0.37391037]\n",
      "[0.46879113]\n",
      "[0.47484475]\n",
      "[0.37550735]\n",
      "[0.51852685]\n",
      "[0.33862394]\n",
      "[0.44136295]\n",
      "[0.44804183]\n",
      "[0.4059124]\n",
      "[0.49614978]\n",
      "[0.48837146]\n",
      "[0.42650643]\n",
      "[0.47874388]\n",
      "[0.42696443]\n",
      "[0.3937799]\n",
      "[0.2593702]\n",
      "[0.47080266]\n",
      "[0.47904506]\n",
      "[0.41025788]\n",
      "[0.4285348]\n",
      "[0.48167008]\n",
      "[0.43083745]\n",
      "[0.37205333]\n",
      "[0.49794763]\n",
      "[0.4318787]\n",
      "[0.54670155]\n",
      "[0.44737455]\n",
      "[0.5095909]\n",
      "[0.3847773]\n",
      "[0.4345754]\n",
      "[0.43199778]\n",
      "[0.41611522]\n",
      "[0.43320632]\n",
      "[0.43491915]\n",
      "[0.34824425]\n",
      "[0.691199]\n",
      "[0.46326664]\n",
      "[0.47075993]\n",
      "[0.4718467]\n",
      "[0.00310978]\n",
      "[0.3873861]\n",
      "[0.49663684]\n",
      "[0.40956676]\n",
      "[0.3549113]\n",
      "[0.4215986]\n",
      "[0.45042798]\n",
      "[0.4650157]\n",
      "[0.41894767]\n",
      "[0.4279135]\n",
      "[0.00024721]\n",
      "[0.38600004]\n",
      "[0.44975588]\n",
      "[0.44070873]\n",
      "[0.45666352]\n",
      "[0.14831844]\n",
      "[0.52824515]\n",
      "[0.4043909]\n",
      "[0.46620205]\n",
      "[0.3040509]\n",
      "[0.49827492]\n",
      "[0.31592923]\n",
      "[0.41610044]\n",
      "[0.5157594]\n",
      "[0.38717636]\n",
      "[0.3459731]\n",
      "[0.26339382]\n",
      "[0.27437115]\n",
      "[0.49201027]\n",
      "[0.4554264]\n",
      "[0.37267846]\n",
      "[0.40950918]\n",
      "[0.3261826]\n",
      "[0.22622177]\n",
      "[0.27931857]\n",
      "[0.4681494]\n",
      "[0.36074483]\n",
      "[0.52372515]\n",
      "[0.3715334]\n",
      "[0.29347903]\n",
      "[0.30235565]\n",
      "[0.04861376]\n",
      "[0.33180207]\n",
      "[0.4344651]\n",
      "[0.37554085]\n",
      "[0.39875197]\n",
      "[0.25022227]\n",
      "[0.38183224]\n",
      "[0.52661353]\n",
      "[0.39897367]\n",
      "[0.4161336]\n",
      "[0.46455774]\n",
      "[0.4539873]\n",
      "[0.43133965]\n",
      "[0.23615214]\n",
      "[0.41385546]\n",
      "[0.28207022]\n",
      "[0.4603274]\n",
      "[0.4244022]\n",
      "[0.19144541]\n",
      "[0.4218273]\n",
      "[0.38129282]\n",
      "[0.40743023]\n",
      "[0.5220144]\n",
      "[0.42585164]\n",
      "[0.43825445]\n",
      "[0.42003262]\n",
      "[0.33477056]\n",
      "[0.48031953]\n",
      "[0.40066653]\n",
      "[0.43120274]\n",
      "[0.3585883]\n",
      "[0.30267847]\n",
      "[0.42884004]\n",
      "[0.3657089]\n",
      "[0.36185634]\n",
      "[0.28579873]\n",
      "[0.4167844]\n",
      "[0.3346746]\n",
      "[0.42582697]\n",
      "[0.36954057]\n",
      "[0.31741923]\n",
      "[0.29850322]\n",
      "[0.4032322]\n",
      "[0.47521314]\n",
      "[0.41425702]\n",
      "[0.44403896]\n",
      "[0.389984]\n",
      "[0.44096887]\n",
      "[0.3903397]\n",
      "[0.40183958]\n",
      "[0.4433394]\n",
      "[0.42930296]\n",
      "[0.39401186]\n",
      "[0.4760486]\n",
      "[0.4277375]\n",
      "[0.34129673]\n",
      "[0.27640188]\n",
      "[0.5934657]\n",
      "[0.5348002]\n",
      "[0.00548559]\n",
      "[0.43711597]\n",
      "[0.44459087]\n",
      "[0.37894467]\n",
      "[0.4379843]\n",
      "[0.44002885]\n",
      "[0.4520063]\n",
      "[0.46204817]\n",
      "[0.15415093]\n",
      "[0.52175283]\n",
      "[0.43817335]\n",
      "[0.44348785]\n",
      "[0.37336218]\n",
      "[0.43998203]\n",
      "[0.41712597]\n",
      "[0.42343035]\n",
      "[0.42066944]\n",
      "[0.3522728]\n",
      "[0.3816279]\n",
      "[0.36286798]\n",
      "[0.4128567]\n",
      "[0.44139317]\n",
      "[0.3862188]\n",
      "[0.4007559]\n",
      "[0.4469565]\n",
      "[0.45051476]\n",
      "[0.37261635]\n",
      "[0.39272213]\n",
      "[0.30905882]\n",
      "[0.48375845]\n",
      "[0.31904382]\n",
      "[0.4974444]\n",
      "[0.46417302]\n",
      "[0.45331895]\n",
      "[0.3322612]\n",
      "[0.39752385]\n",
      "[0.46017468]\n",
      "[0.4490223]\n",
      "[0.4818072]\n",
      "[0.37831578]\n",
      "[0.48810884]\n",
      "[0.45862982]\n",
      "[0.40209585]\n",
      "[0.5121825]\n",
      "[0.40399683]\n",
      "[0.48836136]\n",
      "[0.42053849]\n",
      "[0.4798477]\n",
      "[0.44755378]\n",
      "[0.4822984]\n",
      "[0.38327834]\n",
      "[0.4692276]\n",
      "[0.44328067]\n",
      "[0.44495037]\n",
      "[0.3823158]\n",
      "[0.47941855]\n",
      "[0.27103513]\n",
      "[0.5790717]\n",
      "[0.45826966]\n",
      "[0.3204866]\n",
      "[0.28995103]\n",
      "[0.4037965]\n",
      "[0.31570184]\n",
      "[0.34988517]\n",
      "[0.42414245]\n",
      "[0.47372904]\n",
      "[0.42833015]\n",
      "[0.3549763]\n",
      "[0.47976467]\n",
      "[0.43149397]\n",
      "[0.39426154]\n",
      "[0.09840575]\n",
      "[0.46469554]\n",
      "[0.4429354]\n",
      "[0.40670863]\n",
      "[0.42299777]\n",
      "[0.44512358]\n",
      "[0.4455708]\n",
      "[0.42071307]\n",
      "[0.4177516]\n",
      "[0.45754907]\n",
      "[0.42642516]\n",
      "[0.46630386]\n",
      "[0.44629902]\n",
      "[0.47609124]\n",
      "[0.4544111]\n",
      "[0.2742822]\n",
      "[0.3978791]\n",
      "[0.5240187]\n",
      "[0.48781672]\n",
      "[0.40330282]\n",
      "[0.39958853]\n",
      "[0.5073331]\n",
      "[0.4081211]\n",
      "[0.49325442]\n",
      "[0.35979396]\n",
      "[0.4549223]\n",
      "[0.3676049]\n",
      "[0.42257094]\n",
      "[0.37601992]\n",
      "[0.42804977]\n",
      "[0.5135553]\n",
      "[0.28689802]\n",
      "[0.47316763]\n",
      "[0.48842043]\n",
      "[0.404086]\n",
      "[0.49811226]\n",
      "[0.36909366]\n",
      "[0.39001286]\n",
      "[0.40294796]\n",
      "[0.3502251]\n",
      "[0.38747895]\n",
      "[0.30890518]\n",
      "[0.3775798]\n",
      "[0.4349457]\n",
      "[0.4106045]\n",
      "[0.5124925]\n",
      "[0.39714047]\n",
      "[0.43424597]\n",
      "[0.38744503]\n",
      "[0.31097174]\n",
      "[0.35111558]\n",
      "[0.6343829]\n",
      "[0.36597463]\n",
      "[0.43521264]\n",
      "[0.43116498]\n",
      "[0.41335544]\n",
      "[0.27504426]\n",
      "[0.41954318]\n",
      "[0.47768867]\n",
      "[0.40006074]\n",
      "[0.4748972]\n",
      "[0.46327597]\n",
      "[0.27500197]\n",
      "[0.51583844]\n",
      "[0.46327215]\n",
      "[0.47748682]\n",
      "[0.39116096]\n",
      "[0.4326864]\n",
      "[0.2403951]\n",
      "[0.3290887]\n",
      "[0.44540042]\n",
      "[0.40449184]\n",
      "[0.4194527]\n",
      "[0.43450186]\n",
      "[0.42260882]\n",
      "[0.41368377]\n",
      "[0.40511847]\n",
      "[0.42757922]\n",
      "[0.50274396]\n",
      "[0.4682665]\n",
      "[0.37850413]\n",
      "[0.39111358]\n",
      "[0.2955755]\n",
      "[0.42726147]\n",
      "[0.35157168]\n",
      "[0.40404966]\n",
      "[0.28369075]\n",
      "[0.41717443]\n",
      "[0.3619784]\n",
      "[0.477893]\n",
      "[0.27805948]\n",
      "[0.3839071]\n",
      "[0.42461827]\n",
      "[0.6254404]\n",
      "[0.51077014]\n",
      "[0.40445858]\n",
      "[0.37038332]\n",
      "[0.4715352]\n",
      "[0.46232277]\n",
      "[0.39726]\n",
      "[0.24618256]\n",
      "[0.4800988]\n",
      "[0.37715003]\n",
      "[0.38156053]\n",
      "[0.38937756]\n",
      "[0.46275792]\n",
      "[0.45736715]\n",
      "[0.2777189]\n",
      "[0.47911423]\n",
      "[0.51919025]\n",
      "[0.4616188]\n",
      "[0.4989189]\n",
      "[0.37322745]\n",
      "[0.44907382]\n",
      "[0.39290327]\n",
      "[0.57622087]\n",
      "[0.46533975]\n",
      "[0.38994744]\n",
      "[0.35380986]\n",
      "[0.38449368]\n",
      "[0.44361234]\n",
      "[0.3304828]\n",
      "[0.4633819]\n",
      "[0.48831686]\n",
      "[0.41611308]\n",
      "[0.454981]\n",
      "[0.29120332]\n",
      "[0.40110213]\n",
      "[0.41790333]\n",
      "[0.11047599]\n",
      "[0.38225293]\n",
      "[0.25772774]\n",
      "[0.31920576]\n",
      "[0.3113494]\n",
      "[0.3276031]\n",
      "[0.41287893]\n",
      "[0.43572605]\n",
      "[0.38856214]\n",
      "[0.48002952]\n",
      "[0.40334606]\n",
      "[0.36290488]\n",
      "[0.39649168]\n",
      "[0.37842494]\n",
      "[0.49201414]\n",
      "[0.5581028]\n",
      "[0.43834418]\n",
      "[0.42371356]\n",
      "[0.5246128]\n",
      "[0.4011282]\n",
      "[0.36728817]\n",
      "[0.48236752]\n",
      "[0.45416492]\n",
      "[0.48466724]\n",
      "[0.5420623]\n",
      "[0.31226048]\n",
      "[0.4230339]\n",
      "[0.3929417]\n",
      "[0.46003675]\n",
      "[0.51551056]\n",
      "[0.44982722]\n",
      "[0.30912763]\n",
      "[0.39164853]\n",
      "[0.42175192]\n",
      "[0.435374]\n",
      "[0.44081756]\n",
      "[0.3906909]\n",
      "[0.52880156]\n",
      "[0.4505551]\n",
      "[0.42893896]\n",
      "[0.5342783]\n",
      "[0.37640297]\n",
      "[0.36809844]\n",
      "[0.40693825]\n",
      "[0.38571185]\n",
      "[0.43273246]\n",
      "[0.47465897]\n",
      "[0.35634708]\n",
      "[0.45049456]\n",
      "[0.5664724]\n",
      "[0.5023807]\n",
      "[0.5172353]\n",
      "[0.45710498]\n",
      "[0.41591167]\n",
      "[0.426843]\n",
      "[0.5476361]\n",
      "[0.45849738]\n",
      "[0.41401395]\n",
      "[0.5374219]\n",
      "[0.37975147]\n",
      "[0.35810846]\n",
      "[0.4677742]\n",
      "[0.41667947]\n",
      "[0.41110647]\n",
      "[0.43130296]\n",
      "[0.44975287]\n",
      "[0.4779283]\n",
      "[0.44619414]\n",
      "[0.366901]\n",
      "[0.3025277]\n",
      "[0.3823431]\n",
      "[0.5076373]\n",
      "[0.39451057]\n",
      "[0.49341625]\n",
      "[0.4205922]\n",
      "[0.37976867]\n",
      "[0.44508404]\n",
      "[0.38467956]\n",
      "[0.39982665]\n",
      "[0.44284195]\n",
      "[0.47742635]\n",
      "[0.42211896]\n",
      "[0.37765902]\n",
      "[0.01556122]\n",
      "[0.45728415]\n",
      "[0.40621367]\n",
      "[0.43939567]\n",
      "[0.3684783]\n",
      "[0.36011177]\n",
      "[0.4070599]\n",
      "[0.28576064]\n",
      "[0.51001966]\n",
      "[0.490311]\n",
      "[0.41386187]\n",
      "[0.41266832]\n",
      "[0.42986906]\n",
      "[0.37249207]\n",
      "[0.54238504]\n",
      "[0.38677582]\n",
      "[0.4112647]\n",
      "[0.47760904]\n",
      "[0.45391288]\n",
      "[0.43677184]\n",
      "[0.44006467]\n",
      "[0.36217073]\n",
      "[0.33719313]\n",
      "[0.32174426]\n",
      "[0.47329992]\n",
      "[0.5430278]\n",
      "[0.3450116]\n",
      "[0.49328035]\n",
      "[0.37385935]\n",
      "[0.4628792]\n",
      "[0.38393456]\n",
      "[0.4732832]\n",
      "[0.48604426]\n",
      "[0.43498808]\n",
      "[0.4365566]\n",
      "[0.36454684]\n",
      "[0.39812142]\n",
      "[0.51749414]\n",
      "[0.4646348]\n",
      "[0.4203196]\n",
      "[0.41849783]\n",
      "[0.4438965]\n",
      "[0.42700115]\n",
      "[0.40555215]\n",
      "[0.5211812]\n",
      "[0.42804378]\n",
      "[0.3458243]\n",
      "[0.4149848]\n",
      "[0.39772987]\n",
      "[0.41330722]\n",
      "[0.42780876]\n",
      "[0.417374]\n",
      "[0.36481464]\n",
      "[0.31387925]\n",
      "[0.39745986]\n",
      "[0.3708963]\n",
      "[0.44729024]\n",
      "[0.47971103]\n",
      "[0.4426564]\n",
      "[0.49755746]\n",
      "[0.43327686]\n",
      "[0.35036027]\n",
      "[0.36795187]\n",
      "[0.4562323]\n",
      "[0.36712927]\n",
      "[0.45462197]\n",
      "[0.4347033]\n",
      "[0.03863591]\n",
      "[0.39758983]\n",
      "[0.39125988]\n",
      "[0.28635284]\n",
      "[0.31451157]\n",
      "[0.34915674]\n",
      "[0.38592052]\n",
      "[0.19770709]\n",
      "[0.39826557]\n",
      "[0.39349696]\n",
      "[0.35279796]\n",
      "[0.47966924]\n",
      "[0.44623178]\n",
      "[0.2847891]\n",
      "[0.32736236]\n",
      "[0.34712216]\n",
      "[0.50879]\n",
      "[0.36966377]\n",
      "[0.07374683]\n",
      "[0.4587262]\n",
      "[0.20151845]\n",
      "[0.3718735]\n",
      "[0.5464804]\n",
      "[0.4114508]\n",
      "[0.5316154]\n",
      "[0.46862483]\n",
      "[0.4199832]\n",
      "[0.4044383]\n",
      "[0.46580812]\n",
      "[0.41978127]\n",
      "[0.46164447]\n",
      "[0.39235955]\n",
      "[0.45523453]\n",
      "[0.3246383]\n",
      "[0.47980374]\n",
      "[0.4572603]\n",
      "[0.49858657]\n",
      "[0.3564044]\n",
      "[0.42839247]\n",
      "[0.3488606]\n",
      "[0.23133954]\n",
      "[0.37668514]\n",
      "[0.46430066]\n",
      "[0.4555291]\n",
      "[0.45086384]\n",
      "[0.33785343]\n",
      "[0.41219908]\n",
      "[0.31495845]\n",
      "[0.2863706]\n",
      "[0.4156174]\n",
      "[0.3791343]\n",
      "[0.45196083]\n",
      "[0.48327968]\n",
      "[0.39484644]\n",
      "[0.5713711]\n",
      "[0.33857667]\n",
      "[0.46109825]\n",
      "[0.47773227]\n",
      "[0.18650362]\n",
      "[0.38325402]\n",
      "[0.42166296]\n",
      "[0.41572672]\n",
      "[0.33301345]\n",
      "[0.3297228]\n",
      "[0.40802965]\n",
      "[0.38319668]\n",
      "[0.29142898]\n",
      "[0.362984]\n",
      "[0.44701552]\n",
      "[0.3559443]\n",
      "[0.400939]\n",
      "[0.44259486]\n",
      "[0.3529831]\n",
      "[0.4532658]\n",
      "[0.44000515]\n",
      "[0.40345547]\n",
      "[0.44940525]\n",
      "[0.46473232]\n",
      "[0.5258987]\n",
      "[0.5296316]\n",
      "[0.43137917]\n",
      "[0.35448855]\n",
      "[0.4018222]\n",
      "[0.40034428]\n",
      "[0.3285179]\n",
      "[0.57424253]\n",
      "[0.2942922]\n",
      "[0.46239713]\n",
      "[0.44265458]\n",
      "[0.30063552]\n",
      "[0.4927662]\n",
      "[0.41483155]\n",
      "[0.39529324]\n",
      "[0.47176886]\n",
      "[0.45738262]\n",
      "[0.44424275]\n",
      "[0.4756487]\n",
      "[0.29968655]\n",
      "[0.5032126]\n",
      "[0.54180664]\n",
      "[0.44387174]\n",
      "[0.45724362]\n",
      "[0.36859435]\n",
      "[0.38955343]\n",
      "[0.34163946]\n",
      "[0.45002836]\n",
      "[0.44447178]\n",
      "[0.45416838]\n",
      "[0.26408178]\n",
      "[0.45108297]\n",
      "[0.37466824]\n",
      "[0.42495936]\n",
      "[0.4897421]\n",
      "[0.38619244]\n",
      "[0.43125558]\n",
      "[0.4790691]\n",
      "[0.48467574]\n",
      "[0.3721333]\n",
      "[0.43443352]\n",
      "[0.4516886]\n",
      "[0.42586258]\n",
      "[0.30842465]\n",
      "[0.10502717]\n",
      "[0.43658072]\n",
      "[0.53201914]\n",
      "[0.43079674]\n",
      "[0.44926023]\n",
      "[0.42149]\n",
      "[0.48604408]\n",
      "[0.3243853]\n",
      "[0.26928955]\n",
      "[0.34060287]\n",
      "[0.50823367]\n",
      "[0.42547238]\n",
      "[0.43062067]\n",
      "[0.29869616]\n",
      "[0.33914602]\n",
      "[0.48914233]\n",
      "[0.34507683]\n",
      "[0.43203846]\n",
      "[0.43428898]\n",
      "[0.48723856]\n",
      "[0.4481288]\n",
      "[0.25909904]\n",
      "[0.41674253]\n",
      "[0.46536335]\n",
      "[0.5002243]\n",
      "[0.48153612]\n",
      "[0.38049045]\n",
      "[0.4388453]\n",
      "[0.51392114]\n",
      "[0.33466953]\n",
      "[0.38154975]\n",
      "[0.38770947]\n",
      "[0.27526486]\n",
      "[0.39567438]\n",
      "[0.42131937]\n",
      "[0.45298392]\n",
      "[0.44908085]\n",
      "[0.36060572]\n",
      "[0.47671762]\n",
      "[0.40102834]\n",
      "[0.5016499]\n",
      "[0.5111719]\n",
      "[0.4165273]\n",
      "[0.33158568]\n",
      "[0.4701882]\n",
      "[0.41451144]\n",
      "[0.3737551]\n",
      "[0.37103254]\n",
      "[0.3681057]\n",
      "[0.38079834]\n",
      "[0.4014281]\n",
      "[0.38341022]\n",
      "[0.42589113]\n",
      "[0.29730296]\n",
      "[0.5021292]\n",
      "[0.3208924]\n",
      "[0.43940696]\n",
      "[0.489634]\n",
      "[0.42821902]\n",
      "[0.38145602]\n",
      "[0.316507]\n",
      "[0.42959678]\n",
      "[0.4938822]\n",
      "[0.4173835]\n",
      "[0.34744197]\n",
      "[0.43329495]\n",
      "[0.47751665]\n",
      "[0.47850764]\n",
      "[0.42781004]\n",
      "[0.30155867]\n",
      "[0.41451615]\n",
      "[0.32111695]\n",
      "[0.44949418]\n",
      "[0.3866796]\n",
      "[0.4121894]\n",
      "[0.28392655]\n",
      "[0.43055746]\n",
      "[0.5081692]\n",
      "[0.3052708]\n",
      "[0.44274807]\n",
      "[0.39498612]\n",
      "[0.47237498]\n",
      "[0.44363973]\n",
      "[0.41177177]\n",
      "[0.5741004]\n",
      "[0.44238278]\n",
      "[0.41838297]\n",
      "[0.43661797]\n",
      "[0.44894668]\n",
      "[0.40089375]\n",
      "[0.4290018]\n",
      "[0.44722795]\n",
      "[0.46001482]\n",
      "[0.4901684]\n",
      "[0.33081663]\n",
      "[0.4332649]\n",
      "[0.42178512]\n",
      "[0.4462841]\n",
      "[0.3768456]\n",
      "[0.31731123]\n",
      "[0.48184443]\n",
      "[0.37901524]\n",
      "[0.5877974]\n",
      "[0.3380769]\n",
      "[0.4843199]\n",
      "[0.36561567]\n",
      "[0.46800154]\n",
      "[0.5032855]\n",
      "[0.3679284]\n",
      "[0.40400392]\n",
      "[0.40764582]\n",
      "[0.4607575]\n",
      "[0.47528788]\n",
      "[0.4190749]\n",
      "[0.5606563]\n",
      "[0.44648883]\n",
      "[0.30709577]\n",
      "[0.4405772]\n",
      "[0.02837601]\n",
      "[0.428253]\n",
      "[0.41657355]\n",
      "[0.34509736]\n",
      "[0.45879138]\n",
      "[0.4366153]\n",
      "[0.41844115]\n",
      "[0.44101596]\n",
      "[0.565248]\n",
      "[0.3297958]\n",
      "[0.4378263]\n",
      "[0.40250096]\n",
      "[0.39951319]\n",
      "[0.45155752]\n",
      "[0.4727432]\n",
      "[0.38740963]\n",
      "[0.476569]\n",
      "[0.28985304]\n",
      "[0.29836452]\n",
      "[0.45248216]\n",
      "[0.3233]\n",
      "[0.33527333]\n",
      "[0.51845634]\n",
      "[0.47956938]\n",
      "[0.38852516]\n",
      "[0.38384226]\n",
      "[0.38005632]\n",
      "[0.3820399]\n",
      "[0.44934642]\n",
      "[0.42128807]\n",
      "[0.49798283]\n",
      "[0.32749104]\n",
      "[0.35847867]\n",
      "[0.4465249]\n",
      "[0.38367164]\n",
      "[0.39761367]\n",
      "[0.4742232]\n",
      "[0.32211125]\n",
      "[0.42581826]\n",
      "[0.45735508]\n",
      "[0.48699504]\n",
      "[0.31612557]\n",
      "[0.3821234]\n",
      "[0.5241655]\n",
      "[0.4349402]\n",
      "[0.46967438]\n",
      "[0.3212027]\n",
      "[0.56305665]\n",
      "[0.4959315]\n",
      "[0.4336192]\n",
      "[0.4190874]\n",
      "[0.3389258]\n",
      "[0.3658583]\n",
      "[0.44747686]\n",
      "[0.44801748]\n",
      "[0.34685737]\n",
      "[0.4668212]\n",
      "[0.3692782]\n",
      "[0.44267142]\n",
      "[0.5068448]\n",
      "[0.43310818]\n",
      "[0.49311775]\n",
      "[0.41154754]\n",
      "[0.35145336]\n",
      "[0.49440467]\n",
      "[0.3122294]\n",
      "[0.37616593]\n",
      "[0.49147347]\n",
      "[0.42464292]\n",
      "[0.5855115]\n",
      "[0.3430506]\n",
      "[0.4013667]\n",
      "[0.3882357]\n",
      "[0.17012134]\n",
      "[0.41718113]\n",
      "[0.42745936]\n",
      "[0.38471293]\n",
      "[0.5434609]\n",
      "[0.4481098]\n",
      "[0.3867256]\n",
      "[0.4362259]\n",
      "[0.36574236]\n",
      "[0.4057595]\n",
      "[0.31655258]\n",
      "[0.42433307]\n",
      "[0.3905767]\n",
      "[0.4262678]\n",
      "[0.43288743]\n",
      "[0.43238044]\n",
      "[0.4377594]\n",
      "[0.33923334]\n",
      "[0.42523625]\n",
      "[0.3947507]\n",
      "[0.50632393]\n",
      "[0.49662417]\n",
      "[0.50640416]\n",
      "[0.44358397]\n",
      "[0.43130535]\n",
      "[0.54801893]\n",
      "[0.42087063]\n",
      "[0.42723775]\n",
      "[0.39550632]\n",
      "[0.44744185]\n",
      "[0.44287935]\n",
      "[0.4040306]\n",
      "[0.4461193]\n",
      "[0.41102016]\n",
      "[0.48032507]\n",
      "[0.42921698]\n",
      "[0.00053453]\n",
      "[0.4017186]\n",
      "[0.46498427]\n",
      "[0.47078672]\n",
      "[0.63430893]\n",
      "[0.40718195]\n",
      "[0.44311434]\n",
      "[0.400541]\n",
      "[0.48494625]\n",
      "[0.43569738]\n",
      "[0.34996647]\n",
      "[0.29211873]\n",
      "[0.47572792]\n",
      "[0.38919896]\n",
      "[0.38778555]\n",
      "[0.39976817]\n",
      "[0.48482004]\n",
      "[0.46313143]\n",
      "[0.53384465]\n",
      "[0.3958816]\n",
      "[0.41680703]\n",
      "[0.4630723]\n",
      "[0.3481288]\n",
      "[0.44885337]\n",
      "[0.48579362]\n",
      "[0.4127996]\n",
      "[0.41189277]\n",
      "[0.4051234]\n",
      "[0.5207479]\n",
      "[0.34379673]\n",
      "[0.41947073]\n",
      "[0.487819]\n",
      "[0.4137299]\n",
      "[0.40045512]\n",
      "[0.38497764]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(y) for y in my_model.predict(X_test)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49828338594262855"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "y_pred = my_model.predict(X_test)\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(Y_test[:, 1], y_pred[:,1])\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "auc_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='area = {:.3f}'.format(auc_keras))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
