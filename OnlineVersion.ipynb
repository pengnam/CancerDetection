{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6de38c0ff88b6738c5c8751c722c9b489602d88"
   },
   "source": [
    "# Baseline Keras CNN with 160k samples\n",
    "Heavily inspired by https://www.kaggle.com/hrmello/cnn-classification-80-accuracy\n",
    "\n",
    "Thanks to @Marsh for https://www.kaggle.com/vbookshelf/cnn-how-to-use-160-000-images-without-crashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from glob import glob \n",
    "from skimage.io import imread\n",
    "import gc\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d06b98f87cfa19e2548b0d3a558128d563942e1b"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "base_tile_dir = 'input/subset_data/train/'\n",
    "df = pd.DataFrame({'path': glob(os.path.join(base_tile_dir,'*.tif'))})\n",
    "df['id'] = df.path.map(lambda x: x.split('/')[3].split(\".\")[0])\n",
    "labels = pd.read_csv(\"input/subset_data/train_labels_full.csv\")\n",
    "df_data = df.merge(labels, on = \"id\", how=\"left\")\n",
    "\n",
    "# removing this image because it caused a training error previously\n",
    "df_data = df_data[df_data['id'] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2']\n",
    "\n",
    "# removing this image because it's black\n",
    "df_data = df_data[df_data['id'] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0bfaeb9d367e1b720fd81623e9248c3ff7bb0b5e"
   },
   "source": [
    "# Split X and y in train/test and build folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "d60413a93a292379227e2b8979d2abeed70ed718"
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 1500 # load 80k negative examples\n",
    "\n",
    "# take a random sample of class 0 with size equal to num samples in class 1\n",
    "df_0 = df_data[df_data['label'] == 0].sample(SAMPLE_SIZE, random_state = 101)\n",
    "# filter out class 1\n",
    "df_1 = df_data[df_data['label'] == 1].sample(SAMPLE_SIZE, random_state = 101)\n",
    "\n",
    "# concat the dataframes\n",
    "df_data = shuffle(pd.concat([df_0, df_1], axis=0).reset_index(drop=True))\n",
    "\n",
    "# train_test_split # stratify=y creates a balanced validation set.\n",
    "y = df_data['label']\n",
    "df_train, df_val = train_test_split(df_data, test_size=0.10, random_state=101, stratify=y)\n",
    "\n",
    "# Create directories\n",
    "train_path = 'base_dir/train'\n",
    "valid_path = 'base_dir/valid'\n",
    "test_path = '../input/test'\n",
    "for fold in [train_path, valid_path]:\n",
    "    for subf in [\"0\", \"1\"]:\n",
    "        os.makedirs(os.path.join(fold, subf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "e85d943cfc261cca54b34550554549244f947400"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>922e5756b9c44480c5f16a4ccb31c64da4079e00</th>\n",
       "      <td>input/subset_data/train/922e5756b9c44480c5f16a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e566d8ec93ace7664340230e186d4950738a57ac</th>\n",
       "      <td>input/subset_data/train/e566d8ec93ace766434023...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cc3f88c975016c14a6d88cdd1c575050a4c22101</th>\n",
       "      <td>input/subset_data/train/cc3f88c975016c14a6d88c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b990d05fcba904602f1bb1a9d1c0c74c23da1ded</th>\n",
       "      <td>input/subset_data/train/b990d05fcba904602f1bb1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354cbe2a30225b4888156fcaa2e9500b1c2ed5f6</th>\n",
       "      <td>input/subset_data/train/354cbe2a30225b4888156f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                       path  \\\n",
       "id                                                                                            \n",
       "922e5756b9c44480c5f16a4ccb31c64da4079e00  input/subset_data/train/922e5756b9c44480c5f16a...   \n",
       "e566d8ec93ace7664340230e186d4950738a57ac  input/subset_data/train/e566d8ec93ace766434023...   \n",
       "cc3f88c975016c14a6d88cdd1c575050a4c22101  input/subset_data/train/cc3f88c975016c14a6d88c...   \n",
       "b990d05fcba904602f1bb1a9d1c0c74c23da1ded  input/subset_data/train/b990d05fcba904602f1bb1...   \n",
       "354cbe2a30225b4888156fcaa2e9500b1c2ed5f6  input/subset_data/train/354cbe2a30225b4888156f...   \n",
       "\n",
       "                                          label  \n",
       "id                                               \n",
       "922e5756b9c44480c5f16a4ccb31c64da4079e00      0  \n",
       "e566d8ec93ace7664340230e186d4950738a57ac      1  \n",
       "cc3f88c975016c14a6d88cdd1c575050a4c22101      1  \n",
       "b990d05fcba904602f1bb1a9d1c0c74c23da1ded      1  \n",
       "354cbe2a30225b4888156fcaa2e9500b1c2ed5f6      1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the id as the index in df_data\n",
    "df_data.set_index('id', inplace=True)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "c6683c986522a604dc1a715687b1d1c621628e94"
   },
   "outputs": [],
   "source": [
    "for image in df_train['id'].values:\n",
    "    # the id in the csv file does not have the .tif extension therefore we add it here\n",
    "    fname = image + '.tif'\n",
    "    label = str(df_data.loc[image,'label']) # get the label for a certain image\n",
    "    src = os.path.join('input/subset_data/train', fname)\n",
    "    dst = os.path.join(train_path, label, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "for image in df_val['id'].values:\n",
    "    fname = image + '.tif'\n",
    "    label = str(df_data.loc[image,'label']) # get the label for a certain image\n",
    "    src = os.path.join('input/subset_data/train', fname)\n",
    "    dst = os.path.join(valid_path, label, fname)\n",
    "    shutil.copyfile(src, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "145ef9177524908eb5656fd1deb55a82aeb01973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2700 images belonging to 2 classes.\n",
      "Found 300 images belonging to 2 classes.\n",
      "Found 300 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "IMAGE_SIZE = 96\n",
    "num_train_samples = len(df_train)\n",
    "num_val_samples = len(df_val)\n",
    "train_batch_size = 32\n",
    "val_batch_size = 32\n",
    "\n",
    "train_steps = np.ceil(num_train_samples / train_batch_size)\n",
    "val_steps = np.ceil(num_val_samples / val_batch_size)\n",
    "\n",
    "datagen = ImageDataGenerator(preprocessing_function=lambda x:(x - x.mean()) / x.std() if x.std() > 0 else x,\n",
    "                            horizontal_flip=True,\n",
    "                            vertical_flip=True)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(train_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=train_batch_size,\n",
    "                                        class_mode='binary')\n",
    "\n",
    "val_gen = datagen.flow_from_directory(valid_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=val_batch_size,\n",
    "                                        class_mode='binary')\n",
    "\n",
    "# Note: shuffle=False causes the test dataset to not be shuffled\n",
    "test_gen = datagen.flow_from_directory(valid_path,\n",
    "                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                        batch_size=1,\n",
    "                                        class_mode='binary',\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "037766b4d6d71232ff280ee04aee45671db93b7a"
   },
   "source": [
    "# Define the model \n",
    "**Model structure (optimizer: Adam):**\n",
    "\n",
    "* In \n",
    "* [Conv2D*3 -> MaxPool2D -> Dropout] x3 --> (filters = 16, 32, 64)\n",
    "* Flatten \n",
    "* Dense (256) \n",
    "* Dropout \n",
    "* Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "1d252eb588aaf888171ff82b332efd4a0880cab3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.layers import Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "kernel_size = (3,3)\n",
    "pool_size= (2,2)\n",
    "first_filters = 32\n",
    "second_filters = 64\n",
    "third_filters = 128\n",
    "\n",
    "dropout_conv = 0.3\n",
    "dropout_dense = 0.5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(first_filters, kernel_size, activation = 'relu', input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)))\n",
    "model.add(Conv2D(first_filters, kernel_size, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(pool_size = pool_size)) \n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Conv2D(second_filters, kernel_size, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(second_filters, kernel_size, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(pool_size = pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Conv2D(third_filters, kernel_size, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(third_filters, kernel_size, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(pool_size = pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "#model.add(GlobalAveragePooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(dropout_dense))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(Adam(0.01), loss = \"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "56a7cce49809eae68d588a542620e16368e4fd1a"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a6dc621f8fc4f4558cb22289986a4886fe9e64c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/13\n",
      "85/85 [==============================] - 156s 2s/step - loss: 0.6419 - acc: 0.7008 - val_loss: 0.6624 - val_acc: 0.5933\n",
      "Epoch 2/13\n",
      "85/85 [==============================] - 158s 2s/step - loss: 0.5601 - acc: 0.7347 - val_loss: 1.1403 - val_acc: 0.5700\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 3/13\n",
      "66/85 [======================>.......] - ETA: 33s - loss: 0.5057 - acc: 0.7604"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1, restore_best_weights=True)\n",
    "reducel = ReduceLROnPlateau(monitor='val_loss', patience=1, verbose=1, factor=0.1)\n",
    "history = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=13,\n",
    "                   callbacks=[reducel, earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c8099fc4994ae9792c74477711794c83db3aeee"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make a prediction\n",
    "y_pred_keras = model.predict_generator(test_gen, steps=len(df_val), verbose=1)\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(test_gen.classes, y_pred_keras)\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "auc_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f755d935752d007d21a59ac111cd0d604880c5fa"
   },
   "source": [
    "# Plot ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2397d1c4956ea8615140b9cc40bb857e8a5deeae"
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='area = {:.3f}'.format(auc_keras))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d52bccbbd7d4a8114b5bc9f53ce26db2601fa238"
   },
   "source": [
    "# Load test data and predict\n",
    "I could not find a smart way to do this without crashing the Kernel (due to MemoryError). So I just load the test files in batches, predict, and concatenate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1ccdefc4770fa3b4d57c56990105173161471420"
   },
   "outputs": [],
   "source": [
    "base_test_dir = '../input/test/'\n",
    "test_files = glob(os.path.join(base_test_dir,'*.tif'))\n",
    "submission = pd.DataFrame()\n",
    "file_batch = 5000\n",
    "max_idx = len(test_files)\n",
    "for idx in range(0, max_idx, file_batch):\n",
    "    print(\"Indexes: %i - %i\"%(idx, idx+file_batch))\n",
    "    test_df = pd.DataFrame({'path': test_files[idx:idx+file_batch]})\n",
    "    test_df['id'] = test_df.path.map(lambda x: x.split('/')[3].split(\".\")[0])\n",
    "    test_df['image'] = test_df['path'].map(imread)\n",
    "    K_test = np.stack(test_df[\"image\"].values)\n",
    "    K_test = (K_test - K_test.mean()) / K_test.std()\n",
    "    predictions = model.predict(K_test)\n",
    "    test_df['label'] = predictions\n",
    "    submission = pd.concat([submission, test_df[[\"id\", \"label\"]]])\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0c095619c41eb1996612d96e1ec3b147ebbe601"
   },
   "outputs": [],
   "source": [
    "#submission\n",
    "# Delete the test_dir directory we created to prevent a Kaggle error.\n",
    "# Kaggle allows a max of 500 files to be saved.\n",
    "\n",
    "shutil.rmtree(train_path)\n",
    "shutil.rmtree(valid_path)\n",
    "submission.to_csv(\"submission.csv\", index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "357d239024be4067fa88d87ca0dc0202a59b9d1b"
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
